{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import uisrnn\n",
    "from uisrnn import contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/jovyan/work'\n",
    "# rttm_path = f'{base_dir}/voxsrc21-dia/data/voxconverse/sample/rttm'\n",
    "rttm_path = f'{base_dir}/datasets/voxconverse/test/rttm'\n",
    "sequences_path = f'{base_dir}/voxsrc21-dia/embeddings/sequences'\n",
    "os.makedirs(sequences_path, exist_ok=True)\n",
    "\n",
    "# speaker assignment should be processed with the audios, but since they weren't\n",
    "# these are from the logs, to replicate the files order on embedding extraction (and interval generation)\n",
    "dev_audio_ids = [\"abjxc\",\"ahnss\",\"akthc\",\"asxwr\",\"atgpi\",\"aufkn\",\"azisu\",\"bkwns\",\"bspxd\",\"bxpwa\",\"cyyxp\",\"czlvt\",\"dhorc\",\"djqif\",\"dscgs\",\"edixl\",\"ehpau\",\"eqttu\",\"esrit\",\"exymw\",\"eziem\",\"ezsgk\",\"femmv\",\"fkvvo\",\"fsaal\",\"fvyvb\",\"fxgvy\",\"ggvel\",\"gocbm\",\"gqbvk\",\"gqdxy\",\"grzbb\",\"hiyis\",\"hqyok\",\"hycgx\",\"ikgcq\",\"imtug\",\"ipqqq\",\"iqbww\",\"iqtde\",\"irvat\",\"jnivh\",\"jynhe\",\"kctgl\",\"kkghn\",\"ktzmw\",\"kuduk\",\"ldkmv\",\"lfzib\",\"lknjp\",\"luvfz\",\"mdbod\",\"mekog\",\"mesob\",\"mgpok\",\"migzj\",\"mkrcv\",\"mpvoh\",\"mqxsf\",\"mwfmq\",\"nctdh\",\"nfqjx\",\"ntchr\",\"nxgad\",\"oekmc\",\"ooxnm\",\"pgkde\",\"plbbw\",\"pnyir\",\"ppgjx\",\"qfdpp\",\"qouur\",\"qppll\",\"qrzjk\",\"qydmg\",\"qygfk\",\"qzwxa\",\"rcxzg\",\"rxgun\",\"sikkm\",\"sosnj\",\"suuxu\",\"syiwe\",\"szsyz\",\"tguxv\",\"tjkfn\",\"tlprc\",\"tplwz\",\"udjij\",\"uexjc\",\"ulriv\",\"uvnmy\",\"vmbga\",\"whmpa\",\"wjhgf\",\"xiglo\",\"xxwgv\",\"ycxxe\",\"ydlfw\",\"ypwjd\",\"yrsve\",\"yuzyu\",\"ywcwr\",\"zcdsd\",\"zfkap\",\"zmndm\",\"zrlyl\",\"zyffh\",\"nnqfq\",\"aisvi\",\"usbgm\",\"xvllq\",\"oenox\",\"praxo\",\"onpra\",\"kefgo\",\"bauzd\",\"mjgil\",\"blwmj\",\"gofnj\",\"uatlu\",\"rtvuw\",\"wnfoi\",\"evtyi\",\"tcwsn\",\"pilgb\",\"cmfyw\",\"dbugl\",\"mevkw\",\"jsdmu\",\"jiqvr\",\"hkzpa\",\"hgeec\",\"jcako\",\"epdpg\",\"cqaec\",\"kkwkn\",\"spzmn\",\"ngyrk\",\"sldwj\",\"cmhsm\",\"ndkwv\",\"kbkon\",\"bdopb\",\"qhesr\",\"cwryz\",\"djngn\",\"dvngl\",\"qsfzo\",\"gwtwd\",\"cobal\",\"sduml\",\"vysqj\",\"jtagk\",\"hgdez\",\"wdjyj\",\"qpylu\",\"tfvyr\",\"falxo\",\"bydui\",\"willh\",\"wspbh\",\"ufpel\",\"kiadt\",\"nrogz\",\"imbqf\",\"crixb\",\"ylnza\",\"wewoz\",\"qvtia\",\"kszpd\",\"bwzyf\",\"xypdm\",\"mvjuk\",\"jhdav\",\"pqmho\",\"jsmbi\",\"ccokr\",\"ampme\",\"odkzj\",\"tiams\",\"tucrg\",\"bravd\",\"houcx\",\"gpjne\",\"goyli\",\"txcok\",\"jyirt\",\"oxxwk\",\"iwdjy\",\"kckqn\",\"ioasm\",\"paibn\",\"kklpv\",\"vbjlx\",\"jyflp\",\"sqkup\",\"xmfzh\",\"afjiv\",\"eapdk\",\"pnook\",\"yfcmz\",\"gzvkx\",\"oklol\",\"qjgpl\",\"wbqza\",\"wmori\",\"ysgbf\",\"zajzs\",\"zidwg\",\"ztzzr\",\"zvmyn\"]\n",
    "test_audio_ids = [\"aepyx\",\"aiqwk\",\"bjruf\",\"bmsyn\",\"bxcfq\",\"byapz\",\"clfcg\",\"cqfmj\",\"crylr\",\"cvofp\",\"dgvwu\",\"dohag\",\"dxbbt\",\"dzsef\",\"eauve\",\"eazeq\",\"eguui\",\"epygx\",\"eqsta\",\"euqef\",\"fijfi\",\"fpfvy\",\"fqrnu\",\"fxnwf\",\"fyqoe\",\"gcfwp\",\"gtjow\",\"gtnjb\",\"gukoa\",\"guvqf\",\"gylzn\",\"gyomp\",\"hcyak\",\"heolf\",\"hhepf\",\"ibrnm\",\"ifwki\",\"iiprr\",\"ikhje\",\"jdrwl\",\"jjkrt\",\"jjvkx\",\"jrfaz\",\"jsbdo\",\"jttar\",\"jxpom\",\"jzkzt\",\"kajfh\",\"kmunk\",\"kpjud\",\"ktvto\",\"kvkje\",\"lbfnx\",\"ledhe\",\"lilfy\",\"ljpes\",\"lkikz\",\"lpola\",\"lscfc\",\"ltgmz\",\"lubpm\",\"luobn\",\"mjmgr\",\"msbyq\",\"mupzb\",\"myjoe\",\"nlvdr\",\"nprxc\",\"ocfop\",\"ofbxh\",\"olzkb\",\"ooxlj\",\"oqwpd\",\"otmpf\",\"ouvtt\",\"poucc\",\"ppexo\",\"pwnsw\",\"qadia\",\"qeejz\",\"qlrry\",\"qwepo\",\"rarij\",\"rmvsh\",\"rxulz\",\"sebyw\",\"sexgc\",\"sfdvy\",\"svxzm\",\"tkybe\",\"tpslg\",\"uedkc\",\"uqxlg\",\"usqam\",\"vncid\",\"vylyk\",\"vzuru\",\"wdvva\",\"wemos\",\"wprog\",\"wwzsk\",\"xggbk\",\"xkgos\",\"xlyov\",\"xmyyy\",\"xqxkt\",\"xtdcl\",\"xtzoq\",\"xvxwv\",\"ybhwz\",\"ylzez\",\"ytmef\",\"yukhy\",\"yzvon\",\"zedtj\",\"zfzlc\",\"zowse\",\"zqidv\",\"zztbo\",\"ralnu\",\"uicid\",\"laoyl\",\"jxydp\",\"pzxit\",\"upshw\",\"gfneh\",\"kzmyi\",\"nkqzr\",\"kgjaa\",\"dkabn\",\"eucfa\",\"erslt\",\"mclsr\",\"fzwtp\",\"dzxut\",\"pkwrt\",\"gmmwm\",\"leneg\",\"sxqvt\",\"pgtkk\",\"fuzfh\",\"vtzqw\",\"rsypp\",\"qxana\",\"optsn\",\"dxokr\",\"ptses\",\"isxwc\",\"gzhwb\",\"mhwyr\",\"duvox\",\"ezxso\",\"jgiyq\",\"rpkso\",\"kmjvh\",\"wcxfk\",\"gcvrb\",\"eddje\",\"pccww\",\"vuewy\",\"tvtoe\",\"oubab\",\"jwggf\",\"aggyz\",\"bidnq\",\"neiye\",\"mkhie\",\"iowob\",\"jbowg\",\"gwloo\",\"uevxo\",\"nitgx\",\"eoyaz\",\"qoarn\",\"mxdpo\",\"auzru\",\"diysk\",\"cwbvu\",\"jeymh\",\"iacod\",\"cawnd\",\"vgaez\",\"bgvvt\",\"tiido\",\"aorju\",\"qajyo\",\"ryken\",\"iabca\",\"tkhgs\",\"tbjqx\",\"mqtep\",\"fowhl\",\"fvhrk\",\"nqcpi\",\"mbzht\",\"uhfrw\",\"utial\",\"cpebh\",\"tnjoh\",\"jsymf\",\"vgevv\",\"mxduo\",\"gkiki\",\"bvyvm\",\"hqhrb\",\"isrps\",\"nqyqm\",\"dlast\",\"pxqme\",\"bpzsc\",\"vdlvr\",\"lhuly\",\"crorm\",\"bvqnu\",\"tpnyf\",\"thnuq\",\"swbnm\",\"cadba\",\"sbrmv\",\"wibky\",\"wlfsf\",\"wwvcs\",\"xffsa\",\"xkmqx\",\"xlsme\",\"ygrip\",\"ylgug\",\"ytula\",\"zehzu\",\"zsgto\",\"zzsba\",\"zzyyo\"]\n",
    "\n",
    "# segments intervals, for creating resulting RTTM and calculating DER and JER\n",
    "# voxcon_dev_intervals = np.load(f'{base_dir}/sequences/voxcon-dev-intervals.npy', allow_pickle=True)\n",
    "# voxcon_test_intervals = np.load(f'{base_dir}/sequences/voxcon-test-intervals.npy', allow_pickle=True)\n",
    "# voxsrc21_intervals = np.load(f'{base_dir}/sequences/voxsrc21-intervals.npy', allow_pickle=True)\n",
    "\n",
    "# train/test_sequences -> list of sequences: M (utterances) * L (segments) * D (embeddings dimension) \n",
    "# train/test_cluster_ids -> list of sequences speakers ids: M * L * 1 (string)\n",
    "voxcon_dev_seqs = np.load(os.path.join(base_dir,'sequences/fixed-voxcon-dev-sequences.npy'), allow_pickle=True).tolist()\n",
    "voxcon_dev_ids = np.load(os.path.join(base_dir,sequences_path,'voxcon-dev-cluster-ids.npy'), allow_pickle=True).tolist()\n",
    "\n",
    "voxcon_test_seqs = np.load(os.path.join(base_dir,'sequences/fixed-voxcon-test-sequences.npy'), allow_pickle=True).tolist()\n",
    "voxcon_test_ids = np.load(os.path.join(base_dir,sequences_path,'voxcon-test-cluster-ids.npy'), allow_pickle=True).tolist()\n",
    "\n",
    "# voxsrc21_seqs = np.load(f'{base_dir}/sequences/voxsrc21-sequences.npy', allow_pickle=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix voxcon dev and test sets for training model, to test on voxsrc21\n",
    "smaller_len = len(voxcon_dev_ids) if len(voxcon_dev_ids) < len(voxcon_test_ids) else len(voxcon_test_ids)\n",
    "\n",
    "dev_seqs = voxcon_dev_seqs[:smaller_len]\n",
    "dev_remain_seqs = voxcon_dev_seqs[smaller_len:]\n",
    "test_seqs = voxcon_test_seqs[:smaller_len]\n",
    "test_remain_seqs = voxcon_test_seqs[smaller_len:]\n",
    "\n",
    "mixed_seqs = np.concatenate(list(zip(dev_seqs, test_seqs))).tolist()\n",
    "mixed_seqs.extend(dev_remain_seqs)\n",
    "mixed_seqs.extend(test_remain_seqs)\n",
    "\n",
    "dev_ids = voxcon_dev_ids[:smaller_len]\n",
    "dev_remain_ids = voxcon_dev_ids[smaller_len:]\n",
    "test_ids = voxcon_test_ids[:smaller_len]\n",
    "test_remain_ids = voxcon_test_ids[smaller_len:]\n",
    "\n",
    "mixed_ids = np.concatenate(list(zip(dev_ids, test_ids))).tolist()\n",
    "mixed_ids.extend(dev_remain_ids)\n",
    "mixed_ids.extend(test_remain_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CRIAR FOLDS AQUI\n",
    "\n",
    "# intervals = dev_intervals\n",
    "audio_ids = dev_audio_ids\n",
    "sequences = voxcon_test_seqs\n",
    "cluster_ids = voxcon_test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import contrib\n",
    "\n",
    "\n",
    "# for m, segments_ids in enumerate(cluster_ids):\n",
    "#     for l, speaker_id in enumerate(segments_ids):\n",
    "#         cluster_ids[m][l] = str(int(re.sub('[^0-9]', \"\", speaker_id))) # int to remove 0 in 00, 01, ...\n",
    "\n",
    "# concat_seq, concat_ids=uisrnn.utils.concatenate_training_data(sequences, cluster_ids)\n",
    "# contrib.estimate_crp_alpha(concat_ids, search_step=0.1)\n",
    "\n",
    "estimated_crp_alpha = 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from contrib import  _get_cdf\n",
    "\n",
    "# cdf = _get_cdf(concat_ids, 1)\n",
    "# cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch.multiprocessing as mp\n",
    "ctx = mp.get_context('forkserver')\n",
    "\n",
    "import uisrnn\n",
    "\n",
    "SAVED_MODEL_NAME = 'voxcon_dev_model.uisrnn'\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "def diarization_experiment(model_args, training_args, inference_args):\n",
    "\n",
    "    train_sequence = np.load('/app/fixed-voxcon-dev-sequences.npy', allow_pickle=True).tolist()\n",
    "    train_cluster_id = np.load('/app/voxsrc21-dia/embeddings/sequences/voxcon-dev-cluster-ids.npy', allow_pickle=True).tolist()\n",
    "\n",
    "    test_sequences = np.load('/app/fixed-voxcon-test-sequences.npy', allow_pickle=True).tolist()\n",
    "    test_cluster_ids = np.load('/app/voxsrc21-dia/embeddings/sequences/voxcon-test-cluster-ids.npy', allow_pickle=True).tolist()\n",
    "\n",
    "    # How many elements each list should have\n",
    "    n = 53\n",
    "\n",
    "    # using list comprehension\n",
    "    split_train_sequence = [train_sequence[i:i + n] for i in range(0, len(train_sequence), n)]\n",
    "    split_train_cluster_id = [train_cluster_id[i:i + n] for i in range(0, len(train_cluster_id), n)]\n",
    "\n",
    "    training_args.train_iteration = 2000\n",
    "    model_args.crp_alpha = 0.76\n",
    "    model = uisrnn.UISRNN(model_args)\n",
    "\n",
    "    # for sequence, cluster_id in zip(split_train_sequence, split_train_cluster_id):\n",
    "        # concatenated_train_sequence = np.concatenate(sequence)\n",
    "        # concatenated_train_cluster_id = np.concatenate(cluster_id)\n",
    "\n",
    "        # Training\n",
    "    #    model.fit(sequence, cluster_id, training_args)\n",
    "    #    model.save(SAVED_MODEL_NAME)\n",
    "\n",
    "    for i in range(4):\n",
    "        model.fit(train_sequence, train_cluster_id, training_args)\n",
    "        model.save(SAVED_MODEL_NAME)\n",
    "\n",
    "    # testing\n",
    "    predicted_cluster_ids = []\n",
    "    test_record = []\n",
    "    # predict sequences in parallel\n",
    "    model.rnn_model.share_memory()\n",
    "    pool = ctx.Pool(NUM_WORKERS, maxtasksperchild=None)\n",
    "    pred_gen = pool.imap(func=partial(model.predict, args=inference_args), iterable=test_sequences[:5])\n",
    "    # collect and score predicitons\n",
    "    for idx, predicted_cluster_id in enumerate(pred_gen):\n",
    "        accuracy = uisrnn.compute_sequence_match_accuracy(test_cluster_ids[idx], predicted_cluster_id)\n",
    "        predicted_cluster_ids.append(predicted_cluster_id)\n",
    "        test_record.append((accuracy, len(test_cluster_ids[idx])))\n",
    "        print('Ground truth labels:')\n",
    "        print(test_cluster_ids[idx])\n",
    "        print('Predicted labels:')\n",
    "        print(predicted_cluster_id)\n",
    "        print('-' * 80)\n",
    "\n",
    "    # close multiprocessing pool\n",
    "    pool.close()\n",
    "\n",
    "    print('Finished diarization experiment')\n",
    "    print(uisrnn.output_result(model_args, training_args, test_record))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"The main function.\"\"\"\n",
    "    model_args, training_args, inference_args = uisrnn.parse_arguments()\n",
    "    print(model_args, training_args, inference_args)\n",
    "    diarization_experiment(model_args, training_args, inference_args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    print('Program completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uisrnn\n",
    "from utils import parse_arguments\n",
    "\n",
    "SAVED_MODEL = '{}/voxsrc21-dia/models/voxcon_dev_model-crp076.uisrnn'.format(base_dir)\n",
    "\n",
    "model_args, training_args, inference_args = parse_arguments()\n",
    "\n",
    "model_args.crp_alpha = 0.76\n",
    "model = uisrnn.UISRNN(model_args)\n",
    "model.load(SAVED_MODEL)\n",
    "\n",
    "labels_uisrnn = model.predict(data, inference_args)\n",
    "\n",
    "annotation = Annotation()\n",
    "annotation.uri = sample_id\n",
    "for jdx, speaker_id in enumerate(labels_uisrnn):\n",
    "    segment_interval = test_intervals[idx][jdx]\n",
    "    annotation[Segment(segment_interval[0], segment_interval[1])] = speaker_id\n",
    "    \n",
    "hypothesis_uisrnn = annotation.support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smooth = 1\n",
    "transit_num = smooth\n",
    "bias_denominator = 2 * smooth\n",
    "for cluster_id_seq in train_cluster_ids:\n",
    "    for entry in range(len(cluster_id_seq) - 1):\n",
    "        transit_num += (cluster_id_seq[entry] != cluster_id_seq[entry + 1])\n",
    "        bias_denominator += 1\n",
    "bias = transit_num / bias_denominator\n",
    "\n",
    "print(bias, bias_denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_center = {\n",
    "    'A': np.array([0.0, 0.0]),\n",
    "    'B': np.array([0.0, 1.0]),\n",
    "    'C': np.array([1.0, 0.0]),\n",
    "    'D': np.array([1.0, 1.0]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # generate training data\n",
    "train_cluster_id = ['A'] * 400 + ['B'] * 300 + ['C'] * 200 + ['D'] * 100\n",
    "random.shuffle(train_cluster_id)\n",
    "train_sequence = _generate_random_sequence(\n",
    "    train_cluster_id, label_to_center, sigma=0.01)\n",
    "train_sequences = [\n",
    "    train_sequence[:100, :],\n",
    "    train_sequence[100:300, :],\n",
    "    train_sequence[300:600, :],\n",
    "    train_sequence[600:, :]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(train_sequences, np.ndarray):\n",
    "    # train_sequences is already the concatenated sequence\n",
    "    print('np.array is bad')\n",
    "elif isinstance(train_sequences, list):\n",
    "    # train_sequences is a list of un-concatenated sequences\n",
    "    # we will concatenate it later, after estimating transition_bias\n",
    "    print('tÃ¡ top pai')\n",
    "    pass\n",
    "else:\n",
    "    raise TypeError('train_sequences must be a list or numpy.ndarray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(train_sequences, list) or not isinstance(train_cluster_ids, list):\n",
    "    raise TypeError('train_sequences and train_cluster_ids must be lists')\n",
    "if len(train_sequences) != len(train_cluster_ids):\n",
    "    raise ValueError('train_sequences and train_cluster_ids must have same size')\n",
    "\n",
    "train_cluster_ids = [x.tolist() if isinstance(x, np.ndarray) else x for x in train_cluster_ids]\n",
    "global_observation_dim = None\n",
    "\n",
    "# print(train_cluster_ids.shape, train_cluster_ids[0].shape)\n",
    "\n",
    "\n",
    "for i, (train_sequence, train_cluster_id) in enumerate(zip(train_sequences, train_cluster_ids)):\n",
    "    train_length, observation_dim = train_sequence.shape\n",
    "    print(train_length,observation_dim)\n",
    "    if i == 0:\n",
    "        global_observation_dim = observation_dim\n",
    "    elif global_observation_dim != observation_dim:\n",
    "        raise ValueError('train_sequences must have consistent observation dimension')\n",
    "    if not isinstance(train_cluster_id, list):\n",
    "        raise TypeError('Elements of train_cluster_ids must be list or numpy.ndarray')\n",
    "    if len(train_cluster_id) != train_length:\n",
    "        raise ValueError('Each train_sequence and its train_cluster_id must have same length')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
