{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59653c7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pyannote/pyannote-audio/archive/master.zip\" to /home/jovyan/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading list of pretrained models and pipelines to \"/home/jovyan/.pyannote/hub/pretrained.yml\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da83678b7684444e8755c123979555ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/901 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pretrained model \"emb_voxceleb\" to \"/home/jovyan/.pyannote/hub/models/emb_voxceleb.zip\".\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453acace1aee4a42a23d03394812db1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pyannote/audio/embedding/approaches/arcface_loss.py:170: FutureWarning: The 's' parameter is deprecated in favor of 'scale', and will be removed in a future release\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding has dimension 512.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pyannote.database import get_protocol, FileFinder\n",
    "\n",
    "# Load embedding model\n",
    "emb = torch.hub.load('pyannote/pyannote-audio', 'emb')\n",
    "print(f'Embedding has dimension {emb.dimension:d}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd775d8",
   "metadata": {},
   "source": [
    "#### Load audio files protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4ab51ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessors = {'audio': FileFinder()}\n",
    "protocol = get_protocol('VOXCON.SpeakerDiarization.Sample', preprocessors=preprocessors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be533f6e",
   "metadata": {},
   "source": [
    "#### Get single file for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee03e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.features.utils import get_audio_duration\n",
    "\n",
    "train_file = next(protocol.train())\n",
    "duration = get_audio_duration(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7684d4",
   "metadata": {},
   "source": [
    "#### Create SlidingWindow for embedding extraction from utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import SlidingWindow, Segment\n",
    "\n",
    "sw = SlidingWindow(duration=0.025, step=0.01, start=0.0, end=duration)\n",
    "\n",
    "# Test SlidingWindow\n",
    "# for chunk in sw(Segment(3, 7.5)):\n",
    "#     print(tuple(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af129f6",
   "metadata": {},
   "source": [
    "### D-Vector Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3446f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_segs(times, segs):\n",
    "    #Concatenate continuous voiced segments\n",
    "    concat_seg = []\n",
    "    seg_concat = segs[0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "    return concat_seg\n",
    "\n",
    "def align_embeddings(embeddings):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    avg_embeddings = np.zeros((len(partitions),512))\n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0) \n",
    "    return avg_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401b06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from hparam import hparam as hp\n",
    "from VAD_segments import VAD_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test and train set from the same folder\n",
    "\n",
    "# audio_path = glob.glob(os.path.dirname(hp.unprocessed_data))  \n",
    "\n",
    "# total_speaker_num = len(audio_path)\n",
    "# train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
    "\n",
    "# audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b4426d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/abjxc.wav'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(protocol.train())['audio'].as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a514720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = []\n",
    "train_cluster_id = []\n",
    "label = 0\n",
    "count = 0\n",
    "train_saved = False\n",
    "listLength = len(list(protocol.train()))\n",
    "for i, file in enumerate(protocol.train()):\n",
    "    filePath = file['audio'].as_posix()\n",
    "    times, segs = VAD_chunk(2, filePath)\n",
    "    if segs == []:\n",
    "        print('No voice activity detected')\n",
    "        continue\n",
    "    concat_seg = concat_segs(times, segs)\n",
    "#     STFT_frames = get_STFTs(concat_seg)\n",
    "#     STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "#     STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "#     print(STFT_frames)\n",
    "    embeddings = emb(file)\n",
    "    aligned_embeddings = align_embeddings(embeddings.data)\n",
    "    train_sequence.append(aligned_embeddings)\n",
    "    for embedding in aligned_embeddings:\n",
    "        train_cluster_id.append(str(label))\n",
    "    count = count + 1\n",
    "    if count % 100 == 0:\n",
    "        print('Processed {0}/{1} files'.format(count, listLength))\n",
    "    label = label + 1\n",
    "    \n",
    "#     if not train_saved and i > train_speaker_num:\n",
    "#         train_sequence = np.concatenate(train_sequence,axis=0)\n",
    "#         train_cluster_id = np.asarray(train_cluster_id)\n",
    "#         np.save('train_sequence',train_sequence)\n",
    "#         np.save('train_cluster_id',train_cluster_id)\n",
    "#         train_saved = True\n",
    "#         train_sequence = []\n",
    "#         train_cluster_id = []\n",
    "        \n",
    "train_sequence = np.concatenate(train_sequence,axis=0)\n",
    "train_cluster_id = np.asarray(train_cluster_id)\n",
    "np.save('test_sequence',train_sequence)\n",
    "np.save('test_cluster_id',train_cluster_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf4579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
