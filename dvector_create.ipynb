{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59653c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pyannote.database import get_protocol, FileFinder\n",
    "\n",
    "emb = torch.hub.load('pyannote/pyannote-audio', 'emb')\n",
    "print(f'Embedding has dimension {emb.dimension:d}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ab51ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessors = {'audio': FileFinder()}\n",
    "protocol = get_protocol('VOXCON.SpeakerDiarization.Sample', preprocessors=preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee03e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.features.utils import get_audio_duration\n",
    "\n",
    "train_file = next(protocol.train())\n",
    "duration = get_audio_duration(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import SlidingWindow, Segment\n",
    "\n",
    "sw = SlidingWindow(duration=0.025, step=0.01, start=0.0, end=duration)\n",
    "\n",
    "for chunk in sw(Segment(3, 7.5)):\n",
    "    print(tuple(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3446f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_segs(times, segs):\n",
    "    #Concatenate continuous voiced segments\n",
    "    concat_seg = []\n",
    "    seg_concat = segs[0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "    return concat_seg\n",
    "\n",
    "def get_STFTs(segs):\n",
    "    #Get 240ms STFT windows with 50% overlap\n",
    "    sr = hp.data.sr\n",
    "    STFT_frames = []\n",
    "    for seg in segs:\n",
    "        S = librosa.core.stft(y=seg, n_fft=hp.data.nfft,\n",
    "                              win_length=int(hp.data.window * sr), hop_length=int(hp.data.hop * sr))\n",
    "        S = np.abs(S)**2\n",
    "        mel_basis = librosa.filters.mel(sr, n_fft=hp.data.nfft, n_mels=hp.data.nmels)\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "        for j in range(0, S.shape[1], int(.12/hp.data.hop)):\n",
    "            if j + 24 < S.shape[1]:\n",
    "                STFT_frames.append(S[:,j:j+24])\n",
    "            else:\n",
    "                break\n",
    "    return STFT_frames\n",
    "\n",
    "def align_embeddings(embeddings):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    avg_embeddings = np.zeros((len(partitions),512))\n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0) \n",
    "    return avg_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b06bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from hparam import hparam as hp\n",
    "from VAD_segments import VAD_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ed718",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = glob.glob(os.path.dirname(hp.unprocessed_data))  \n",
    "\n",
    "total_speaker_num = len(audio_path)\n",
    "train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
    "\n",
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(protocol.train())['audio'].as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a514720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = []\n",
    "train_cluster_id = []\n",
    "label = 0\n",
    "count = 0\n",
    "train_saved = False\n",
    "listLength = len(list(protocol.train()))\n",
    "for i, file in enumerate(protocol.train()):\n",
    "    filePath = file['audio'].as_posix()\n",
    "    times, segs = VAD_chunk(2, filePath)\n",
    "    if segs == []:\n",
    "        print('No voice activity detected')\n",
    "        continue\n",
    "    concat_seg = concat_segs(times, segs)\n",
    "#     STFT_frames = get_STFTs(concat_seg)\n",
    "#     STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "#     STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "#     print(STFT_frames)\n",
    "    embeddings = emb(file)\n",
    "    aligned_embeddings = align_embeddings(embeddings.data)\n",
    "    train_sequence.append(aligned_embeddings)\n",
    "    for embedding in aligned_embeddings:\n",
    "        train_cluster_id.append(str(label))\n",
    "    count = count + 1\n",
    "    if count % 100 == 0:\n",
    "        print('Processed {0}/{1} files'.format(count, listLength))\n",
    "    label = label + 1\n",
    "    \n",
    "#     if not train_saved and i > train_speaker_num:\n",
    "#         train_sequence = np.concatenate(train_sequence,axis=0)\n",
    "#         train_cluster_id = np.asarray(train_cluster_id)\n",
    "#         np.save('train_sequence',train_sequence)\n",
    "#         np.save('train_cluster_id',train_cluster_id)\n",
    "#         train_saved = True\n",
    "#         train_sequence = []\n",
    "#         train_cluster_id = []\n",
    "        \n",
    "train_sequence = np.concatenate(train_sequence,axis=0)\n",
    "train_cluster_id = np.asarray(train_cluster_id)\n",
    "np.save('test_sequence',train_sequence)\n",
    "np.save('test_cluster_id',train_cluster_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef451772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
