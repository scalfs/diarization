{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in data preparation\n",
    "\n",
    "1. Load the audio using librosa\n",
    "2. Get the duration using librosa.get_duration\n",
    "3. Calculate each frame width in ms\n",
    "4. Split the audio on VAD (Below 20db is silence)\n",
    "5. For each split calculate mel (180 frames) \n",
    "6. np.transpose the data Ex: (1,40,180) to (180,1,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(noise_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/noise', train_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/train', test_path='/datadrive2/dalon/diarization-experiments/Speaker_Verification/data/VCTK-Corpus/test', tdsv=False, sr=16000, nfft=512, window=0.025, hop=0.01, tdsv_frame=160, tisv_frame=160, tisv_frame_min=50, hidden=768, proj=256, num_layer=3, restore=False, model_path='./tisv_model', model_num=3, train=False, N=4, M=5, noise_filenum=16, loss='softmax', optim='sgd', lr=0.001, beta1=0.5, beta2=0.9, iteration=100000, comment='', max_batch_utterances=1000)\n",
      "Log path: /home/jovyan/work/voxsrc21-dia/data-prep/data-generation-for-uisrnn.logs\n"
     ]
    }
   ],
   "source": [
    "# All imports\n",
    "import os, sys, logging\n",
    "import datetime\n",
    "import time, shutil, pickle\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from configuration import get_config\n",
    "\n",
    "config = get_config()\n",
    "log_file = os.path.abspath(\"data-generation-for-uisrnn.logs\")\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All configurations below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 222 # random seed\n",
    "config.N = 64 # Number of speakers per batch\n",
    "config.M = 10 # Number of utterences per speaker\n",
    "config.iteration = 50000000 # Number of iterations to run\n",
    "config.lr = 1e-3\n",
    "config.hidden = 768 # hidden state dimension of lstm\n",
    "config.proj = 256 # projection dimension of lstm\n",
    "# config.restore = True\n",
    "config.model_num = 1\n",
    "logging.info(f'N={config.N}, M={config.M}')\n",
    "logging.info(f'Model restore: {config.restore}, Model number: {config.model_num}')\n",
    "\n",
    "# Configurations\n",
    "\n",
    "#_____________ Parameters to tune on dev set _______________________\n",
    "# VAD param\n",
    "# Changing to 25, which will give slightly better intervals, 20 gives very short intervals\n",
    "vad_threshold = 25 # threshold for voice activity detection\n",
    "\n",
    "# Segment param\n",
    "acceptable_shortseg_dur = 0.2 # in second\n",
    "#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "# model parameters\n",
    "model_path = '/home/jovyan/work/voxsrc21-dia/models/model.uisrnn-1' # model save path\n",
    "dataset_path = '/home/jovyan/work/datasets/voxceleb-1/sample/wav'\n",
    "save_dir_path = '/home/jovyan/work/voxsrc21-dia/embeddings'\n",
    "os.makedirs(save_dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm saving only 2 embeddings i.e. first and last tisv_frames for given interval in an audio. So each .npy\n",
    "embedding file will have a shape of (2, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/datasets/voxceleb-1/sample/wav/id10001/.DS_Store\n",
      "/home/jovyan/work/datasets/voxceleb-1/sample/wav/id10001/1zcIwhmdeo4\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150/1184618237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maudio_file_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#hidden folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maudio_file_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_file_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         utter, sr = librosa.core.load(\n\u001b[1;32m     13\u001b[0m             audio_path, sr=config.sr)        # load audio\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Each embedding saved file will have (2, 256)\n",
    "\n",
    "for folder in os.listdir(dataset_path):\n",
    "    speakerid = folder\n",
    "    folder = os.path.join(dataset_path, folder)\n",
    "    # for base_id in os.listdir(data_path):\n",
    "    #     for video_id in os.listdir(os.path.join(data_path, base_id)):\n",
    "    #         if video_id.startswith('.'): #hidden folders\n",
    "    #             continue;\n",
    "    #         for audio_id in os.listdir(os.path.join(data_path, base_id, video_id)):\n",
    "    \n",
    "    for audio_file_name in os.listdir(folder):\n",
    "        audio_path = os.path.join(folder, audio_file_name)\n",
    "        audio_file_number = audio_file_name.split('.')[0].split('_')[1]\n",
    "        utter, sr = librosa.core.load(\n",
    "            audio_path, sr=config.sr)        # load audio\n",
    "        # lower bound of utterance length\n",
    "        utter_min_len = (config.tisv_frame *\n",
    "                         config.hop + config.window) * sr\n",
    "        # Get the duration\n",
    "        duration = librosa.get_duration(utter, sr)\n",
    "        # Duration of each window\n",
    "        duration_per_frame = (duration / utter.shape[0])\n",
    "#             print(f'Duration: {duration}\\nDuration per frame: {duration_per_frame}s\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "        tisv_frame_duration_s = utter_min_len * duration_per_frame\n",
    "        intervals = librosa.effects.split(\n",
    "            utter, top_db=vad_threshold)         # voice activity detection\n",
    "\n",
    "        for idx, current_interval in enumerate(intervals):\n",
    "            utterances_spec = []\n",
    "            # save first and last 160 frames of spectrogram.\n",
    "            utter_part = utter[current_interval[0]:current_interval[1]]\n",
    "            S = librosa.core.stft(y=utter_part, n_fft=config.nfft,\n",
    "                                  win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "            S = np.abs(S) ** 2\n",
    "            mel_basis = librosa.filters.mel(\n",
    "                sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "            # log mel spectrogram of utterances\n",
    "            S = np.log10(np.dot(mel_basis, S) + 1e-6)\n",
    "        #         print(S.shape)\n",
    "            utterances_spec.append(S[:, :config.tisv_frame])\n",
    "            utterances_spec.append(S[:, -config.tisv_frame:])\n",
    "\n",
    "            utterances_spec = np.array(utterances_spec)\n",
    "            utter_batch = np.transpose(utterances_spec, axes=(\n",
    "                2, 0, 1))     # transpose [frames, batch, n_mels]\n",
    "        #         print(utter_batch.shape)\n",
    "\n",
    "            data = sess.run(embedded, feed_dict={verif: utter_batch})\n",
    "            save_embedding_path = os.path.join(\n",
    "                save_dir_path, f'vctk-{speakerid}-{audio_file_number}-{idx}.npy')\n",
    "            np.save(save_embedding_path, data)\n",
    "#                 print(data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structuring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files 93417\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "list_of_embedding_path = sorted(os.listdir(save_dir_path))\n",
    "print(f'Total number of files {len(list_of_embedding_path)}')\n",
    "list_of_embedding_path[:5]\n",
    "\n",
    "embedding_dict = defaultdict(list)\n",
    "\n",
    "for file in list_of_embedding_path:\n",
    "    embedding_dict[file.split('-')[1]].append(os.path.join(save_dir_path, file))\n",
    "\n",
    "for key in embedding_dict:\n",
    "    if len(embedding_dict[key]) < 5: # remove the keys if its embeddings is less than 5\n",
    "        embedding_dict.pop(key)\n",
    "        print(f'Poped {key}')\n",
    "\n",
    "def shuffle_two(dict_of_two, train_sequence_path, train_ids):\n",
    "    \"\"\"Shuffle the given 2 labels\"\"\"\n",
    "    max_to_pick = 5\n",
    "    key1, key2 = dict_of_two.keys()\n",
    "#     print(len(dict_of_two[key1]), len(dict_of_two[key2]))\n",
    "    while dict_of_two[key1] and dict_of_two[key2]:\n",
    "#         print('in while')\n",
    "        no_to_pick = np.random.randint(1, max_to_pick)\n",
    "        if no_to_pick <= len(dict_of_two[key1]):\n",
    "#             print(no_to_pick, len(dict_of_two[key1]))\n",
    "            train_sequence_path.extend(dict_of_two[key1][:no_to_pick])\n",
    "            del dict_of_two[key1][:no_to_pick]\n",
    "            train_ids.extend([key1] * no_to_pick)\n",
    "        else: break\n",
    "        no_to_pick = np.random.randint(1, max_to_pick)\n",
    "        if no_to_pick <= len(dict_of_two[key2]):\n",
    "#             print(no_to_pick)\n",
    "            train_sequence_path.extend(dict_of_two[key2][:no_to_pick])\n",
    "            del dict_of_two[key2][:no_to_pick]\n",
    "            train_ids.extend([key2] * no_to_pick)\n",
    "        else: break\n",
    "    no_to_pick = len(dict_of_two[key1])\n",
    "    train_sequence_path.extend(dict_of_two[key1])\n",
    "    train_ids.extend([key1] * no_to_pick)\n",
    "    no_to_pick = len(dict_of_two[key2])\n",
    "    train_sequence_path.extend(dict_of_two[key2])\n",
    "    train_ids.extend([key2] * no_to_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 93417 93417\n"
     ]
    }
   ],
   "source": [
    "train_sequence_path = []\n",
    "train_ids = []\n",
    "while len(embedding_dict) >= 2:\n",
    "    first2pairs = {k: embedding_dict[k] for k in list(embedding_dict)[:2]}\n",
    "    shuffle_two(first2pairs, train_sequence_path, train_ids)\n",
    "    # remove the keys from embedding_dict\n",
    "    for key in first2pairs:\n",
    "        embedding_dict.pop(key)\n",
    "print(len(embedding_dict), len(train_sequence_path), len(train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_sequence_path[81234:81239], train_ids[81234:81239]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = []\n",
    "train_cluster_id = []\n",
    "for idx, item in enumerate(train_sequence_path):\n",
    "    embeddings = np.load(item)\n",
    "    train_sequence.extend(embeddings.tolist())\n",
    "    train_cluster_id.extend([train_ids[idx], train_ids[idx]])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186834, 186834)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequence), len(train_cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('/datadrive/dalon/diarizer-dataset/VCTK-Corpus/vctk_training_data.npz',\n",
    "         train_sequence=train_sequence, train_cluster_id=train_cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoid = 'zPFptdATk_s'\n",
    "save_utter_label_interval = f'/datadrive/dalon/uis-rnn/Notebooks/{videoid}_5min.b'\n",
    "save_test_data = f'/datadrive/dalon/uis-rnn/Notebooks/{videoid}_test.npz' # it will have embeddings and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_utter_label_interval, 'rb') as f:\n",
    "    _tmp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /datadrive/dalon/models/m-64-10-768-256/Check_Point/model.ckpt-46\n"
     ]
    }
   ],
   "source": [
    "embeddings = []\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, model_path)\n",
    "\n",
    "    utter = _tmp['utter']\n",
    "    sr = config.sr\n",
    "    utter_min_len = (config.tisv_frame * config.hop + config.window) * sr    # lower bound of utterance length\n",
    "    # Get the duration\n",
    "    duration = librosa.get_duration(utter, sr)\n",
    "    # Duration of each window\n",
    "    duration_per_frame = (duration / utter.shape[0])\n",
    "#             print(f'Duration: {duration}\\nDuration per frame: {duration_per_frame}s\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "    tisv_frame_duration_s = utter_min_len * duration_per_frame\n",
    "    intervals = _tmp['intervals']\n",
    "\n",
    "    for idx, current_interval in enumerate(intervals):\n",
    "        utterances_spec = []\n",
    "        utter_part = utter[current_interval[0]:current_interval[1]]         # save first and last 160 frames of spectrogram.\n",
    "        S = librosa.core.stft(y=utter_part, n_fft=config.nfft,\n",
    "                              win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "#         print(S.shape)\n",
    "        utterances_spec.append(S[:, :config.tisv_frame])\n",
    "        utterances_spec.append(S[:, -config.tisv_frame:])\n",
    "\n",
    "        utterances_spec = np.array(utterances_spec)\n",
    "        utter_batch = np.transpose(utterances_spec, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n",
    "        data = sess.run(embedded, feed_dict={verif:utter_batch})\n",
    "        embeddings.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cluster_ids = []\n",
    "for item in _tmp['labels_list']:\n",
    "    test_cluster_ids.extend([item, item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(save_test_data,\n",
    "         test_sequences=embeddings,\n",
    "         test_cluster_ids=test_cluster_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create single npz file for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoids = ['zPFptdATk_s', 'VqF96Um0HQw']\n",
    "save_test_files = []\n",
    "for videoid in videoids:\n",
    "    save_test_files.append(f'/datadrive/dalon/uis-rnn/Notebooks/{videoid}_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = []\n",
    "test_cluster_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512//125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert these into smaller chunks\n",
    "def split_to_chunks(test_sequences, test_cluster_ids, ids, seqs):\n",
    "    max_size = 125\n",
    "    start = 0\n",
    "    for i in range(len(ids) // max_size):\n",
    "        test_sequences.append(seqs[start:max_size * (i + 1)])\n",
    "        test_cluster_ids.append(ids[start:max_size * (i + 1)])\n",
    "        start = max_size * (i + 1)\n",
    "    test_sequences.append(seqs[start:])\n",
    "    test_cluster_ids.append(ids[start:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in save_test_files:\n",
    "    _tmp = np.load(file)\n",
    "    if len(list(_tmp['test_cluster_ids'])) > 150:\n",
    "        split_to_chunks(test_sequences,\n",
    "                        test_cluster_ids,\n",
    "                        list(_tmp['test_cluster_ids']),\n",
    "                        np.float64(_tmp['test_sequences']))\n",
    "    else:\n",
    "        test_sequences.append(np.float64(_tmp['test_sequences']))\n",
    "        test_cluster_ids.append(list(_tmp['test_cluster_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = np.array(test_sequences)\n",
    "test_cluster_ids = np.array(test_cluster_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = '/datadrive/dalon/uis-rnn/data/testing_data_custom_2vid.npz'\n",
    "np.savez(test_data_path,\n",
    "         test_sequences=test_sequences,\n",
    "         test_cluster_ids=test_cluster_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VCTK data prep for UIS-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path to vctk dataset\n",
    "# There are sub folders for each speaker with wav file inside them\n",
    "dataset_path = '/datadrive/dalon/diarizer-dataset/VCTK-Corpus/wav48/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_desc = defaultdict(list) # speaker id is the key and value is list of utterences\n",
    "for folder in os.listdir(dataset_path):\n",
    "    speakerid = folder\n",
    "    folder = os.path.join(dataset_path, folder)\n",
    "    for utter_path in os.listdir(folder):\n",
    "        utter_path = os.path.join(folder, utter_path)\n",
    "        speakers_desc[f'vctk-{speakerid}'].append(utter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_desc = []\n",
    "for key in speakers_desc:\n",
    "#     print(f'{key} = {len(speakers_desc[key])}')\n",
    "    utter_desc.append(len(speakers_desc[key]))\n",
    "utter_desc = np.array(utter_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 503, 405.89908256880733)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utter_desc.min(), utter_desc.max(), utter_desc.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
