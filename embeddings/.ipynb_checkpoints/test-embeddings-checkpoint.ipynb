{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from utils import normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from collections import defaultdict\n",
    "from configuration import get_config\n",
    "from rttm import load_rttm, Turn\n",
    "from VAD_segments import VAD_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "config.log_path = 'voxconverse-sample-embeddings.logs'\n",
    "log_file = os.path.abspath(config.log_path)\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/wav'\n",
    "rttm_path = '/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/rttm'\n",
    "# data_path = '/app/datasets/voxconverse/test/wav'\n",
    "# rttm_path = '/app/datasets/voxconverse/test/rttm'\n",
    "config.model_path = '/home/jovyan/work/voxsrc21-dia/models/model.ckpt-46'\n",
    "save_dir_path = '/home/jovyan/work/voxsrc21-dia/embeddings/sequences/voxconverse-sample'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Data prep\n",
    "# I'm saving only 2 embeddings i.e. first and last tisv_frames for given interval in an audio. So each .npy\n",
    "# embedding file will have a shape of (2, 256)\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2 # Fixing to 2 since we take 2 for each interval #utter_batch.shape[1]\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "config_tensorflow = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_segs(times, segs):\n",
    "    # Concatenate continuous voiced segments\n",
    "    # with segment time information (onset and offset)\n",
    "    concat_seg = []\n",
    "    concat_times=[]\n",
    "    seg_concat = segs[0]\n",
    "    seg_onset = times[0][0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            # If segments are continuous, concatenate them\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            # If not, append a new segment sequence\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "            # Save segment time offset and append a new one\n",
    "            seg_offset = times[i][1]\n",
    "            seg_interval = [seg_onset, seg_offset]\n",
    "            concat_times.append(seg_interval)\n",
    "            seg_onset = times[i+1][0]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "        # Save last time offset\n",
    "        seg_offset = times[i+1][1]\n",
    "        seg_interval = [seg_onset, seg_offset]\n",
    "        concat_times.append(seg_interval)\n",
    "        \n",
    "    return concat_seg, concat_times\n",
    "\n",
    "def get_STFTs(segs, time_segs):\n",
    "    #Get 240ms STFT windows with 50% overlap\n",
    "    sr = config.sr\n",
    "    STFT_windows = []\n",
    "    time_windows = []\n",
    "    for i, seg in enumerate(segs):\n",
    "        S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "        # log mel spectrogram of utterances\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)\n",
    "        \n",
    "        STFT_couple = []\n",
    "        segment_time_onset = time_segs[i][0]\n",
    "        for j in range(0, S.shape[1], int(.12/config.hop)):# 0.24 / 0.01 = 24.0\n",
    "            # if hop != 0.01, we can't use 12 or 24 frames (they stop making sense)\n",
    "            if j + 24 < S.shape[1]:\n",
    "                if len(STFT_couple) < 2:\n",
    "                    # we save the stft in couples because of the expected entry of the lstm embedding network\n",
    "                    # it was trained like this, so its expected by the pre-trained model and weights\n",
    "                    STFT_couple.append(S[:,j:j+24])\n",
    "                else:\n",
    "                    STFT_windows.append(STFT_couple)\n",
    "                    STFT_couple = [S[:,j:j+24]]\n",
    "                    # returns the time intervals for each STFT window\n",
    "                    window_onset = segment_time_onset + 0.01*j\n",
    "                    # the time windows doesn't have to be save in couples\n",
    "                    # after processing, the embeddings and time_windows will have the same length\n",
    "                    time_windows.extend([[window_onset-0.12, window_onset+0.12], [window_onset, window_onset+0.24]])\n",
    "            else:\n",
    "                break\n",
    "    return np.array(STFT_windows), np.array(time_windows)\n",
    "\n",
    "def align_embeddings(embeddings, intervals):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    \n",
    "    avg_embeddings = np.zeros((len(partitions),256))\n",
    "    segment_intervals = [] \n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0)\n",
    "       \n",
    "        partition_interval = intervals[partition[0]:partition[1]] \n",
    "        #start of first partition\n",
    "        interval_onset = partition_interval[0][0]\n",
    "        #end of last partition\n",
    "        partition_offset_idx = -2 if len(partition_interval) > 1 else -1 \n",
    "        interval_offset = partition_interval[partition_offset_idx][1]\n",
    "        \n",
    "        segment_intervals.append([interval_onset, interval_offset])\n",
    "    return avg_embeddings, np.array(segment_intervals)\n",
    "\n",
    "def getOnsets(turn):\n",
    "    return turn.onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unique_extensions = []\n",
    "# Using List as default factory\n",
    "audio_files = defaultdict(list)\n",
    "rttm_files = defaultdict(list)\n",
    "\n",
    "for audio_file in os.listdir(data_path):\n",
    "    if audio_file.startswith('.'): #hidden folders\n",
    "        continue;\n",
    "    audio_id = os.path.splitext(audio_file)[0]\n",
    "    extension = os.path.splitext(audio_file)[1]\n",
    "    all_unique_extensions.append(extension)\n",
    "#     print(f'Audio id: {audio_id}')\n",
    "    if extension == '.wav':\n",
    "        audio_files[audio_id] = os.path.join(data_path, audio_file)\n",
    "        rttm_files[audio_id] = os.path.join(rttm_path, audio_id + '.rttm')\n",
    "    else:\n",
    "        print(f'Wrong file type in {os.path.join(data_path, audio_file)}')\n",
    "\n",
    "audio_quantity = len(audio_files)\n",
    "print(f'Unique file extensions: {set(all_unique_extensions)}')\n",
    "print(f'Number of audios: {audio_quantity}')\n",
    "print(f'Number of rttms: {len(rttm_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, config.model_path)\n",
    "\n",
    "    times, segs = VAD_chunk(2, audio_files.get(audio_id))\n",
    "    concat_seg, concat_times = concat_segs(times, segs)\n",
    "    STFT_windows, time_windows = get_STFTs(concat_seg, concat_times)\n",
    "    # print(len(STFT_windows), STFT_windows[0].shape)\n",
    "\n",
    "    embeddings = np.array([]).reshape(0,256)\n",
    "    for STFT_window in STFT_windows:\n",
    "        STFT_batch = np.transpose(STFT_window, axes=(2,0,1))\n",
    "        # print(STFT_batch.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\n",
    "        embeddings_batch = sess.run(embedded, feed_dict={verif:STFT_batch})\n",
    "        embeddings = np.concatenate((embeddings, embeddings_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aligned_embeddings, segment_intervals = align_embeddings(embeddings, time_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFTs(segs, time_segs):\n",
    "    #Get 240ms STFT windows with 50% overlap\n",
    "    sr = config.sr\n",
    "    STFT_windows = []\n",
    "    time_windows = []\n",
    "    for i, seg in enumerate(segs):\n",
    "        S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "        # log mel spectrogram of utterances\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)\n",
    "        \n",
    "        STFT_couple = []\n",
    "        segment_time_onset = time_segs[i][0]\n",
    "        for j in range(0, S.shape[1], int(.12/config.hop)):# 0.24 / 0.01 = 24.0\n",
    "            # if hop != 0.01, we can't use 12 or 24 frames (they stop making sense)\n",
    "            if j + 24 < S.shape[1]:\n",
    "                if len(STFT_couple) < 2:\n",
    "                    # we save the stft in couples because of the expected entry of the lstm embedding network\n",
    "                    # it was trained like this, so its expected by the pre-trained model and weights\n",
    "                    STFT_couple.append(S[:,j:j+24])\n",
    "                else:\n",
    "                    STFT_windows.append(STFT_couple)\n",
    "                    STFT_couple = [S[:,j:j+24]]\n",
    "                    # returns the time intervals for each STFT window\n",
    "                    window_onset = segment_time_onset + 0.01*j\n",
    "                    # the time windows doesn't have to be save in couples\n",
    "                    # after processing, the embeddings and time_windows will have the same length\n",
    "                    time_windows.extend([[window_onset-0.24, window_onset], [window_onset-0.12, window_onset+0.12]])\n",
    "            else:\n",
    "                break\n",
    "    return np.array(STFT_windows), np.array(time_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for time in concat_times:\n",
    "    print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(segment_intervals)-1):\n",
    "    if (abs(segment_intervals[i][1] - segment_intervals[i+1][0]) > 0.00000001):\n",
    "        print(segment_intervals[i], segment_intervals[i+1],segment_intervals[i][1] - segment_intervals[i+1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_id, rttm_path in rttm_files.items():\n",
    "    _, speakers, _ = load_rttm(rttm_files.get(audio_id)[0])\n",
    "    print(audio_id, len(speakers))\n",
    "\n",
    "turns, _, _ = load_rttm(rttm_files.get(audio_id)[0])\n",
    "\n",
    "print(\n",
    "turns[0].onset,\n",
    "turns[0].offset,\n",
    "turns[0].dur,\n",
    "turns[0].speaker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar com o turns retornado pelo load_rttm para montar o train_cluster_ids\n",
    "turns, _, _ = load_rttm(rttm_files.get(audio_id)[0])\n",
    "turns.sort(key=getOnsets)\n",
    "\n",
    "interval_turn = Turn(0, offset=0.001)\n",
    "for interval in time_windows:\n",
    "    if interval[0] > interval_turn.offset:\n",
    "        interval_turn\n",
    "    train_cluster_ids.append(str(speaker_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, config.model_path)\n",
    "   \n",
    "    audio_count = 0\n",
    "    train_sequences = []\n",
    "    sequence_intervals = []\n",
    "    train_cluster_ids = []\n",
    "    \n",
    "    for audio_id, audio_path in audio_files.items():\n",
    "        # Path: audio_files.get(audio_id)[0]\n",
    "        logging.info(f'loading {audio_id} {audio_count}/{audio_quantity}')\n",
    "\n",
    "        # voice activity detection            \n",
    "        times, segs = VAD_chunk(2, audio_path)\n",
    "        concat_seg, concat_times = concat_segs(times, segs)\n",
    "        STFT_windows, time_windows = get_STFTs(concat_seg, concat_times)\n",
    "        # print(len(STFT_windows), STFT_windows[0].shape)\n",
    "\n",
    "        embeddings = np.array([]).reshape(0,256)\n",
    "        for STFT_window in STFT_windows:\n",
    "            STFT_batch = np.transpose(STFT_window, axes=(2,0,1))\n",
    "            # print(STFT_batch.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\n",
    "            embeddings_batch = sess.run(embedded, feed_dict={verif:STFT_batch})\n",
    "            embeddings = np.concatenate((embeddings, embeddings_batch))\n",
    "            \n",
    "        # Turn window-level embeddings to segment-level (400ms)\n",
    "        aligned_embeddings, segment_intervals = align_embeddings(embeddings, time_windows)\n",
    "        \n",
    "        # Comparar com o turns retornado pelo load_rttm para montar o train_cluster_ids\n",
    "        turns, _, _ = load_rttm(rttm_files.get(audio_id)[0])\n",
    "        for interval in time_windows:\n",
    "            train_cluster_ids.append(str(speaker_count))\n",
    "            \n",
    "        train_sequences.append(aligned_embeddings)\n",
    "        sequence_intervals.append(segment_intervals)\n",
    "\n",
    "        audio_count += 1\n",
    "        \n",
    "        if (audio_count == audio_quantity or audio_count % 20 == 0):\n",
    "            train_sequences_path = os.path.join(save_dir_path, f'voxcon-dev-train-sequences.npy')\n",
    "            np.save(train_sequences_path, train_sequence)\n",
    "            \n",
    "            intervals_path = os.path.join(save_dir_path, f'voxcon-dev-intervals.npy')\n",
    "            np.save(intervals_path, sequence_intervals)\n",
    "            \n",
    "            train_cluster_ids_path = os.path.join(save_dir_path, f'voxcon-dev-train-cluster-ids.npy')\n",
    "            train_cluster_ids = np.asarray(train_cluster_ids)\n",
    "            np.save(train_cluster_ids_path, train_cluster_ids)\n",
    "            logging.info(f'saved train sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = []\n",
    "train_sequence.append(aligned_embeddings)\n",
    "print(len(train_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_STFTs(segs):\n",
    "#     #Get 240ms STFT windows with 50% overlap\n",
    "#     sr = config.sr\n",
    "#     STFT_frames = []\n",
    "#     for seg in segs:\n",
    "#         S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "#         S = np.abs(S) ** 2\n",
    "#         mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "#         # log mel spectrogram of utterances\n",
    "#         S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "#         for j in range(0, S.shape[1], int(.12/config.hop)):\n",
    "#             if j + 24 < S.shape[1]:\n",
    "#                 STFT_frames.append(S[:,j:j+24])\n",
    "#             else:\n",
    "#                 break\n",
    "#     return STFT_frames\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from hparam import hparam as hp\n",
    "\n",
    "# class SpeechEmbedder(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SpeechEmbedder, self).__init__()    \n",
    "#         self.LSTM_stack = nn.LSTM(hp.data.nmels, hp.model.hidden, num_layers=hp.model.num_layer, batch_first=True)\n",
    "#         for name, param in self.LSTM_stack.named_parameters():\n",
    "#           if 'bias' in name:\n",
    "#              nn.init.constant_(param, 0.0)\n",
    "#           elif 'weight' in name:\n",
    "#              nn.init.xavier_normal_(param)\n",
    "#         self.projection = nn.Linear(hp.model.hidden, hp.model.proj)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n",
    "#         #only use last frame\n",
    "#         x = x[:,x.size(1)-1]\n",
    "#         x = self.projection(x.float())\n",
    "#         x = x / torch.norm(x, dim=1).unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "# embedder_net = SpeechEmbedder()\n",
    "# # embedder_net.load_state_dict(torch.load(hp.model.model_path))\n",
    "# embedder_net.eval()\n",
    "\n",
    "# times, segs = VAD_chunk(2, audio_path)\n",
    "# concat_seg = concat_segs(times, segs)\n",
    "# STFT_frames = get_STFTs(concat_seg)\n",
    "# STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "# STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "\n",
    "# embeddings = embedder_net(STFT_frames)\n",
    "\n",
    "# embeddings.shape\n",
    "\n",
    "# STFT_frames.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
