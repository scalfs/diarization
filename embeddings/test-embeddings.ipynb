{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(audio_file='../data/voxconverse/sample/abjxc.wav', hidden=768, hop=0.01, log_path='main.logs', model_num=3, model_path='../models/model.ckpt-46', nfft=512, num_layer=3, number_of_speakers=2, output_dir='output.json', proj=256, random_state=123, restore=False, sr=16000, srt_path='xxx.en.srt', tdsv=False, tdsv_frame=160, tisv_frame=160, tisv_frame_min=50, window=0.025)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from utils import normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from collections import defaultdict\n",
    "from configuration import get_config\n",
    "from rttm import load_rttm\n",
    "from VAD_segments import VAD_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: /home/jovyan/work/voxsrc21-dia/embeddings/voxconverse-sample-embeddings.logs\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config.log_path = 'voxconverse-sample-embeddings.logs'\n",
    "log_file = os.path.abspath(config.log_path)\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/wav'\n",
    "rttm_path = '/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/rttm'\n",
    "# data_path = '/app/datasets/voxconverse/test/wav'\n",
    "# rttm_path = '/app/datasets/voxconverse/test/rttm'\n",
    "save_dir_path = '/home/jovyan/work/voxsrc21-dia/embeddings/sequences/voxconverse-sample'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Data prep\n",
    "# I'm saving only 2 embeddings i.e. first and last tisv_frames for given interval in an audio. So each .npy\n",
    "# embedding file will have a shape of (2, 256)\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2 # Fixing to 2 since we take 2 for each interval #utter_batch.shape[1]\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "config_tensorflow = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_segs(times, segs):\n",
    "    # Concatenate continuous voiced segments\n",
    "    # with segment time information (onset and offset)\n",
    "    concat_seg = []\n",
    "    concat_times=[]\n",
    "    seg_concat = segs[0]\n",
    "    seg_onset = times[0][0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            # If segments are continuous, concatenate them\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            # If not, append a new segment sequence\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "            # Save segment time offset and append a new one\n",
    "            seg_offset = times[i][1]\n",
    "            seg_interval = [seg_onset, seg_offset]\n",
    "            concat_times.append(seg_interval)\n",
    "            seg_onset = times[i+1][0]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "        # Save last time offset\n",
    "        seg_offset = times[i+1][1]\n",
    "        seg_interval = [seg_onset, seg_offset]\n",
    "        concat_times.append(seg_interval)\n",
    "        \n",
    "    return concat_seg, concat_times\n",
    "\n",
    "def get_STFTs(segs, time_segs):\n",
    "    #Get 240ms STFT windows with 50% overlap, in pairs\n",
    "    sr = config.sr\n",
    "    STFT_windows = []\n",
    "    time_windows = []\n",
    "    for i, seg in enumerate(segs):\n",
    "        S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "        # log mel spectrogram of utterances\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)\n",
    "        \n",
    "        # S.shape[1] ~= math.ceil((time_segs[i][1] - time_segs[i][0])*100+1)\n",
    "        segment_time_onset = time_segs[i][0]    \n",
    "        for j in range(0, S.shape[1], int(.24/config.hop)): # 0.24 / 0.01 = 24.0\n",
    "            # if hop != 0.01, we can't use 12, 24, 36 frames (they stop making sense)\n",
    "            # 36 frames are related to .36s of the audio\n",
    "            if j + 36 < S.shape[1]:\n",
    "                # in order to fit on the expected shape of the embedding network we double the window\n",
    "                STFT_windows.append([S[:, j:j+24], S[:, j+12:j+36]])\n",
    "                # returns the time intervals for each STFT window\n",
    "                window_onset = segment_time_onset + 0.01*j\n",
    "                time_windows.extend([[window_onset, window_onset+0.24], [window_onset+0.12, window_onset+0.36]])\n",
    "            else:\n",
    "                break\n",
    "    return np.array(STFT_windows), np.array(time_windows)\n",
    "\n",
    "def align_embeddings(embeddings, intervals):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    \n",
    "    avg_embeddings = np.zeros((len(partitions),256))\n",
    "    segment_intervals = [] \n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0)\n",
    "\n",
    "        partition_interval = intervals[partition[0]:partition[1]]\n",
    "        interval_onset = partition_interval[0][0]   #start of first partition\n",
    "        interval_offset = partition_interval[-1][1] #end of last partition\n",
    "        segment_intervals.append([interval_onset, interval_offset])\n",
    "    return avg_embeddings, np.array(segment_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique file extensions: {'.wav'}\n",
      "Number of audios: 19\n",
      "Number of rttms: 19\n"
     ]
    }
   ],
   "source": [
    "all_unique_extensions = []\n",
    "# Using List as default factory\n",
    "audio_files = defaultdict(list)\n",
    "rttm_files = defaultdict(list)\n",
    "\n",
    "for audio_file in os.listdir(data_path):\n",
    "    if audio_file.startswith('.'): #hidden folders\n",
    "        continue;\n",
    "    audio_id = os.path.splitext(audio_file)[0]\n",
    "    extension = os.path.splitext(audio_file)[1]\n",
    "    all_unique_extensions.append(extension)\n",
    "#     print(f'Audio id: {audio_id}')\n",
    "    if extension == '.wav':\n",
    "        audio_files[audio_id].append(os.path.join(data_path, audio_file))\n",
    "        rttm_files[audio_id].append(os.path.join(rttm_path, audio_id + '.rttm'))\n",
    "    else:\n",
    "        print(f'Wrong file type in {os.path.join(data_path, audio_file)}')\n",
    "\n",
    "audio_quantity = len(audio_files)\n",
    "print(f'Unique file extensions: {set(all_unique_extensions)}')\n",
    "print(f'Number of audios: {audio_quantity}')\n",
    "print(f'Number of rttms: {len(rttm_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abjxc 1\n",
      "afjiv 5\n",
      "ahnss 4\n",
      "aisvi 8\n",
      "akthc 2\n",
      "ampme 3\n",
      "asxwr 3\n",
      "atgpi 1\n",
      "aufkn 3\n",
      "azisu 4\n",
      "bauzd 5\n",
      "bdopb 7\n",
      "bkwns 2\n",
      "blwmj 2\n",
      "bravd 3\n",
      "bspxd 3\n",
      "bwzyf 4\n",
      "bxpwa 5\n",
      "bydui 3\n"
     ]
    }
   ],
   "source": [
    "for audio_id, rttm_path in rttm_files.items():\n",
    "    _, speakers, _ = load_rttm(rttm_files.get(audio_id)[0])\n",
    "    print(audio_id, len(speakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spk00'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turns, _, _ = load_rttm(rttm_files.get('abjxc')[0])\n",
    "\n",
    "turns[0].onset\n",
    "turns[0].offset\n",
    "turns[0].dur\n",
    "turns[0].speaker_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "times, segs = VAD_chunk(2, audio_files.get(audio_id)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_seg, concat_times = concat_segs(times, segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "STFT_windows, time_windows = get_STFTs(concat_seg, concat_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/model.ckpt-46\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, config.model_path)\n",
    "   \n",
    "    embeddings = np.array([]).reshape(0,256)\n",
    "    for idx, STFT_window in enumerate(STFT_windows):\n",
    "        STFT_batch = np.transpose(STFT_window, axes=(2,0,1))\n",
    "        # print(STFT_batch.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\n",
    "        embeddings_batch = sess.run(embedded, feed_dict={verif:STFT_batch})\n",
    "        embeddings = np.concatenate((embeddings, embeddings_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aligned_embeddings, segment_intervals = align_embeddings(embeddings, time_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99.56 99.92] [100.6  100.96] -0.6799999999999926\n",
      "[104.2  104.56] [104.76 105.24] -0.20000000000001705\n",
      "[107.16 107.64] [108.08 108.44] -0.4399999999999977\n",
      "[129.68 130.04] [130.32 130.8 ] -0.28000000000000114\n",
      "[181.32 181.68] [181.84 182.32] -0.1599999999999966\n",
      "[206.92 207.28] [207.96 208.32] -0.6799999999999784\n",
      "[230.88 231.24] [231.4  231.76] -0.1599999999999966\n",
      "[253.5  253.98] [254.4  254.76] -0.4199999999999875\n",
      "[264.   264.36] [264.7  265.18] -0.339999999999975\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(segment_intervals)-1):\n",
    "    if (abs(segment_intervals[i][1] - segment_intervals[i+1][0]) > 0.001):\n",
    "        print(segment_intervals[i], segment_intervals[i+1],segment_intervals[i][1] - segment_intervals[i+1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.04, 0.44),\n",
       " (0.44, 0.84),\n",
       " (0.84, 1.24),\n",
       " (1.24, 1.64),\n",
       " (1.64, 2.04),\n",
       " (2.04, 2.44),\n",
       " (2.44, 2.84),\n",
       " (2.84, 3.24),\n",
       " (3.24, 3.64),\n",
       " (3.64, 4.04),\n",
       " (4.04, 4.44),\n",
       " (4.44, 4.84),\n",
       " (4.84, 5.24),\n",
       " (5.24, 5.64),\n",
       " (5.64, 6.04),\n",
       " (6.04, 6.44),\n",
       " (6.44, 6.84),\n",
       " (6.84, 7.24),\n",
       " (7.24, 7.64),\n",
       " (7.64, 8.04),\n",
       " (8.04, 8.44),\n",
       " (8.44, 8.84),\n",
       " (8.84, 9.24),\n",
       " (9.24, 9.64),\n",
       " (9.64, 10.04),\n",
       " (10.04, 10.44),\n",
       " (10.44, 10.84),\n",
       " (10.84, 11.24),\n",
       " (11.24, 11.64),\n",
       " (11.64, 12.04),\n",
       " (12.04, 12.44),\n",
       " (12.44, 12.84),\n",
       " (12.84, 13.24),\n",
       " (13.24, 13.64),\n",
       " (13.64, 14.04),\n",
       " (14.04, 14.44),\n",
       " (14.44, 14.84),\n",
       " (14.84, 15.24),\n",
       " (15.24, 15.64),\n",
       " (15.64, 16.04),\n",
       " (16.04, 16.44),\n",
       " (16.44, 16.84),\n",
       " (16.84, 17.24),\n",
       " (17.24, 17.64),\n",
       " (17.64, 18.04),\n",
       " (18.04, 18.44),\n",
       " (18.44, 18.84),\n",
       " (18.84, 19.24),\n",
       " (19.24, 19.64),\n",
       " (19.64, 20.04),\n",
       " (20.04, 20.44),\n",
       " (20.44, 20.84),\n",
       " (20.84, 21.24),\n",
       " (21.24, 21.64),\n",
       " (21.64, 22.04),\n",
       " (22.04, 22.44),\n",
       " (22.44, 22.84),\n",
       " (22.84, 23.24),\n",
       " (23.24, 23.64),\n",
       " (23.64, 24.04),\n",
       " (24.04, 24.44),\n",
       " (24.44, 24.84),\n",
       " (24.84, 25.24),\n",
       " (25.24, 25.64),\n",
       " (25.64, 26.04),\n",
       " (26.04, 26.44),\n",
       " (26.44, 26.84),\n",
       " (26.84, 27.24),\n",
       " (27.24, 27.64),\n",
       " (27.64, 28.04),\n",
       " (28.04, 28.44),\n",
       " (28.44, 28.84),\n",
       " (28.84, 29.24),\n",
       " (29.24, 29.64),\n",
       " (29.64, 30.04),\n",
       " (30.04, 30.44),\n",
       " (30.44, 30.84),\n",
       " (30.84, 31.24),\n",
       " (31.24, 31.5),\n",
       " (31.72, 32.12),\n",
       " (32.12, 32.52),\n",
       " (32.52, 32.92),\n",
       " (32.92, 33.32),\n",
       " (33.32, 33.72),\n",
       " (33.72, 34.12),\n",
       " (34.12, 34.52),\n",
       " (34.52, 34.92),\n",
       " (34.92, 35.32),\n",
       " (35.32, 35.72),\n",
       " (35.72, 36.12),\n",
       " (36.12, 36.52),\n",
       " (36.52, 36.92),\n",
       " (36.92, 37.32),\n",
       " (37.32, 37.72),\n",
       " (37.72, 38.12),\n",
       " (38.12, 38.52),\n",
       " (38.52, 38.92),\n",
       " (38.92, 39.32),\n",
       " (39.32, 39.72),\n",
       " (39.72, 40.12),\n",
       " (40.12, 40.52),\n",
       " (40.52, 40.92),\n",
       " (40.92, 41.32),\n",
       " (41.32, 41.72),\n",
       " (41.72, 42.12),\n",
       " (42.12, 42.52),\n",
       " (42.52, 42.92),\n",
       " (42.92, 43.32),\n",
       " (43.32, 43.72),\n",
       " (43.72, 44.12),\n",
       " (44.12, 44.52),\n",
       " (44.52, 44.92),\n",
       " (44.92, 45.32),\n",
       " (45.32, 45.72),\n",
       " (45.72, 46.12),\n",
       " (46.12, 46.4),\n",
       " (46.46, 46.86),\n",
       " (46.86, 47.26),\n",
       " (47.26, 47.66),\n",
       " (47.66, 48.06),\n",
       " (48.06, 48.46),\n",
       " (48.46, 48.86),\n",
       " (48.86, 49.26),\n",
       " (49.26, 49.66),\n",
       " (49.66, 50.06),\n",
       " (50.06, 50.46),\n",
       " (50.46, 50.86),\n",
       " (50.86, 51.26),\n",
       " (51.26, 51.66),\n",
       " (51.66, 52.06),\n",
       " (52.06, 52.46),\n",
       " (52.46, 52.86),\n",
       " (52.86, 53.26),\n",
       " (53.26, 53.66),\n",
       " (53.66, 54.06),\n",
       " (54.06, 54.46),\n",
       " (54.46, 54.86),\n",
       " (54.86, 55.26),\n",
       " (55.26, 55.66),\n",
       " (55.66, 56.06),\n",
       " (56.06, 56.46),\n",
       " (56.46, 56.86),\n",
       " (56.86, 57.26),\n",
       " (57.26, 57.66),\n",
       " (57.66, 58.06),\n",
       " (58.06, 58.46),\n",
       " (58.46, 58.86),\n",
       " (58.86, 59.26),\n",
       " (59.26, 59.66),\n",
       " (59.66, 60.06),\n",
       " (60.06, 60.46),\n",
       " (60.46, 60.86),\n",
       " (60.86, 61.26),\n",
       " (61.26, 61.66),\n",
       " (61.66, 62.06),\n",
       " (62.06, 62.46),\n",
       " (62.46, 62.86),\n",
       " (62.86, 63.26),\n",
       " (63.26, 63.66),\n",
       " (63.66, 64.06),\n",
       " (64.06, 64.46),\n",
       " (64.46, 64.86),\n",
       " (64.86, 65.26),\n",
       " (65.26, 65.66),\n",
       " (65.66, 66.06),\n",
       " (66.06, 66.46),\n",
       " (66.46, 66.86),\n",
       " (66.86, 67.26),\n",
       " (67.26, 67.66),\n",
       " (67.66, 68.06),\n",
       " (68.06, 68.46),\n",
       " (68.46, 68.86),\n",
       " (68.86, 69.26),\n",
       " (69.26, 69.66),\n",
       " (69.66, 70.06),\n",
       " (70.06, 70.46),\n",
       " (70.46, 70.86),\n",
       " (70.86, 71.26),\n",
       " (71.26, 71.66),\n",
       " (71.66, 72.06),\n",
       " (72.06, 72.46),\n",
       " (72.46, 72.86),\n",
       " (72.86, 73.26),\n",
       " (73.26, 73.66),\n",
       " (73.66, 74.06),\n",
       " (74.06, 74.46),\n",
       " (74.46, 74.86),\n",
       " (74.86, 75.26),\n",
       " (75.26, 75.66),\n",
       " (75.66, 76.06),\n",
       " (76.06, 76.46),\n",
       " (76.46, 76.86),\n",
       " (76.86, 77.26),\n",
       " (77.26, 77.66),\n",
       " (77.66, 78.06),\n",
       " (78.06, 78.46),\n",
       " (78.46, 78.86),\n",
       " (78.86, 79.26),\n",
       " (79.26, 79.66),\n",
       " (79.66, 80.06),\n",
       " (80.06, 80.46),\n",
       " (80.46, 80.86),\n",
       " (80.86, 81.26),\n",
       " (81.26, 81.66),\n",
       " (81.66, 82.06),\n",
       " (82.06, 82.46),\n",
       " (82.46, 82.86),\n",
       " (82.86, 83.26),\n",
       " (83.26, 83.66),\n",
       " (83.66, 84.06),\n",
       " (84.06, 84.46),\n",
       " (84.46, 84.86),\n",
       " (84.86, 85.26),\n",
       " (85.26, 85.66),\n",
       " (85.66, 86.06),\n",
       " (86.06, 86.46),\n",
       " (86.46, 86.86),\n",
       " (86.86, 87.26),\n",
       " (87.26, 87.66),\n",
       " (87.66, 88.06),\n",
       " (88.06, 88.46),\n",
       " (88.46, 88.86),\n",
       " (88.86, 89.26),\n",
       " (89.26, 89.66),\n",
       " (89.66, 90.06),\n",
       " (90.06, 90.46),\n",
       " (90.46, 90.86),\n",
       " (90.86, 91.26),\n",
       " (91.26, 91.66),\n",
       " (91.66, 92.06),\n",
       " (92.06, 92.46),\n",
       " (92.46, 92.86),\n",
       " (92.86, 93.26),\n",
       " (93.26, 93.66),\n",
       " (93.66, 94.06),\n",
       " (94.06, 94.46),\n",
       " (94.46, 94.86),\n",
       " (94.86, 95.26),\n",
       " (95.26, 95.66),\n",
       " (95.66, 96.06),\n",
       " (96.06, 96.46),\n",
       " (96.46, 96.86),\n",
       " (96.86, 97.26),\n",
       " (97.26, 97.66),\n",
       " (97.66, 97.94),\n",
       " (98.0, 98.4),\n",
       " (98.4, 98.8),\n",
       " (98.8, 99.2),\n",
       " (99.2, 99.6),\n",
       " (99.6, 100.0),\n",
       " (100.0, 100.22),\n",
       " (100.6, 101.0),\n",
       " (101.0, 101.4),\n",
       " (101.4, 101.8),\n",
       " (101.8, 102.2),\n",
       " (102.2, 102.6),\n",
       " (102.6, 103.0),\n",
       " (103.0, 103.4),\n",
       " (103.4, 103.8),\n",
       " (103.8, 104.2),\n",
       " (104.2, 104.6),\n",
       " (104.6, 104.62),\n",
       " (104.64, 105.04),\n",
       " (105.04, 105.44),\n",
       " (105.44, 105.84),\n",
       " (105.84, 106.24),\n",
       " (106.24, 106.64),\n",
       " (106.64, 107.04),\n",
       " (107.04, 107.44),\n",
       " (107.44, 107.68),\n",
       " (107.96, 108.36),\n",
       " (108.36, 108.76),\n",
       " (108.76, 109.16),\n",
       " (109.16, 109.56),\n",
       " (109.56, 109.96),\n",
       " (109.96, 110.36),\n",
       " (110.36, 110.76),\n",
       " (110.76, 111.16),\n",
       " (111.16, 111.56),\n",
       " (111.56, 111.96),\n",
       " (111.96, 112.36),\n",
       " (112.36, 112.76),\n",
       " (112.76, 113.16),\n",
       " (113.16, 113.56),\n",
       " (113.56, 113.96),\n",
       " (113.96, 114.36),\n",
       " (114.36, 114.76),\n",
       " (114.76, 115.16),\n",
       " (115.16, 115.56),\n",
       " (115.56, 115.96),\n",
       " (115.96, 116.36),\n",
       " (116.36, 116.76),\n",
       " (116.76, 117.16),\n",
       " (117.16, 117.56),\n",
       " (117.56, 117.96),\n",
       " (117.96, 118.36),\n",
       " (118.36, 118.76),\n",
       " (118.76, 119.16),\n",
       " (119.16, 119.56),\n",
       " (119.56, 119.96),\n",
       " (119.96, 120.36),\n",
       " (120.36, 120.76),\n",
       " (120.76, 121.16),\n",
       " (121.16, 121.56),\n",
       " (121.56, 121.96),\n",
       " (121.96, 122.36),\n",
       " (122.36, 122.76),\n",
       " (122.76, 123.16),\n",
       " (123.16, 123.56),\n",
       " (123.56, 123.68),\n",
       " (123.68, 124.08),\n",
       " (124.08, 124.48),\n",
       " (124.48, 124.88),\n",
       " (124.88, 125.28),\n",
       " (125.28, 125.68),\n",
       " (125.68, 126.08),\n",
       " (126.08, 126.48),\n",
       " (126.48, 126.88),\n",
       " (126.88, 127.28),\n",
       " (127.28, 127.68),\n",
       " (127.68, 128.08),\n",
       " (128.08, 128.48),\n",
       " (128.48, 128.88),\n",
       " (128.88, 129.28),\n",
       " (129.28, 129.68),\n",
       " (129.68, 130.08),\n",
       " (130.08, 130.22),\n",
       " (130.32, 130.72),\n",
       " (130.72, 131.12),\n",
       " (131.12, 131.52),\n",
       " (131.52, 131.92),\n",
       " (131.92, 132.32),\n",
       " (132.32, 132.72),\n",
       " (132.72, 133.12),\n",
       " (133.12, 133.52),\n",
       " (133.52, 133.92),\n",
       " (133.92, 134.32),\n",
       " (134.32, 134.72),\n",
       " (134.72, 135.12),\n",
       " (135.12, 135.52),\n",
       " (135.52, 135.92),\n",
       " (135.92, 136.32),\n",
       " (136.32, 136.72),\n",
       " (136.72, 137.12),\n",
       " (137.12, 137.52),\n",
       " (137.52, 137.92),\n",
       " (137.92, 138.32),\n",
       " (138.32, 138.72),\n",
       " (138.72, 139.12),\n",
       " (139.12, 139.52),\n",
       " (139.52, 139.92),\n",
       " (139.92, 140.32),\n",
       " (140.32, 140.72),\n",
       " (140.72, 141.12),\n",
       " (141.12, 141.52),\n",
       " (141.52, 141.92),\n",
       " (141.92, 142.32),\n",
       " (142.32, 142.72),\n",
       " (142.72, 143.12),\n",
       " (143.12, 143.52),\n",
       " (143.52, 143.92),\n",
       " (143.92, 144.32),\n",
       " (144.32, 144.72),\n",
       " (144.72, 145.12),\n",
       " (145.12, 145.52),\n",
       " (145.52, 145.92),\n",
       " (145.92, 146.32),\n",
       " (146.32, 146.72),\n",
       " (146.72, 147.12),\n",
       " (147.12, 147.52),\n",
       " (147.52, 147.92),\n",
       " (147.92, 148.32),\n",
       " (148.32, 148.72),\n",
       " (148.72, 149.12),\n",
       " (149.12, 149.52),\n",
       " (149.52, 149.92),\n",
       " (149.92, 150.32),\n",
       " (150.32, 150.72),\n",
       " (150.72, 151.12),\n",
       " (151.12, 151.52),\n",
       " (151.52, 151.92),\n",
       " (151.92, 152.32),\n",
       " (152.32, 152.72),\n",
       " (152.72, 153.12),\n",
       " (153.12, 153.52),\n",
       " (153.52, 153.92),\n",
       " (153.92, 154.32),\n",
       " (154.32, 154.72),\n",
       " (154.72, 155.12),\n",
       " (155.12, 155.52),\n",
       " (155.52, 155.92),\n",
       " (155.92, 156.32),\n",
       " (156.32, 156.72),\n",
       " (156.72, 157.12),\n",
       " (157.12, 157.52),\n",
       " (157.52, 157.92),\n",
       " (157.92, 158.32),\n",
       " (158.32, 158.72),\n",
       " (158.72, 159.12),\n",
       " (159.12, 159.52),\n",
       " (159.52, 159.92),\n",
       " (159.92, 160.32),\n",
       " (160.32, 160.72),\n",
       " (160.72, 161.12),\n",
       " (161.12, 161.52),\n",
       " (161.52, 161.92),\n",
       " (161.92, 162.32),\n",
       " (162.32, 162.72),\n",
       " (162.72, 163.12),\n",
       " (163.12, 163.52),\n",
       " (163.52, 163.92),\n",
       " (163.92, 164.32),\n",
       " (164.32, 164.72),\n",
       " (164.72, 165.12),\n",
       " (165.12, 165.52),\n",
       " (165.52, 165.92),\n",
       " (165.92, 166.32),\n",
       " (166.32, 166.72),\n",
       " (166.72, 167.12),\n",
       " (167.12, 167.52),\n",
       " (167.52, 167.92),\n",
       " (167.92, 168.32),\n",
       " (168.32, 168.72),\n",
       " (168.72, 169.12),\n",
       " (169.12, 169.52),\n",
       " (169.52, 169.92),\n",
       " (169.92, 170.32),\n",
       " (170.32, 170.72),\n",
       " (170.72, 171.12),\n",
       " (171.12, 171.52),\n",
       " (171.52, 171.92),\n",
       " (171.92, 172.32),\n",
       " (172.32, 172.72),\n",
       " (172.72, 173.12),\n",
       " (173.12, 173.52),\n",
       " (173.52, 173.92),\n",
       " (173.92, 174.32),\n",
       " (174.32, 174.72),\n",
       " (174.72, 175.12),\n",
       " (175.12, 175.52),\n",
       " (175.52, 175.92),\n",
       " (175.92, 176.32),\n",
       " (176.32, 176.72),\n",
       " (176.72, 177.12),\n",
       " (177.12, 177.52),\n",
       " (177.52, 177.92),\n",
       " (177.92, 178.32),\n",
       " (178.32, 178.72),\n",
       " (178.72, 179.12),\n",
       " (179.12, 179.52),\n",
       " (179.52, 179.92),\n",
       " (179.92, 180.32),\n",
       " (180.32, 180.72),\n",
       " (180.72, 181.12),\n",
       " (181.12, 181.52),\n",
       " (181.52, 181.82),\n",
       " (181.84, 182.24),\n",
       " (182.24, 182.64),\n",
       " (182.64, 183.04),\n",
       " (183.04, 183.44),\n",
       " (183.44, 183.84),\n",
       " (183.84, 184.24),\n",
       " (184.24, 184.64),\n",
       " (184.64, 185.04),\n",
       " (185.04, 185.44),\n",
       " (185.44, 185.84),\n",
       " (185.84, 185.98),\n",
       " (186.04, 186.44),\n",
       " (186.44, 186.84),\n",
       " (186.84, 187.24),\n",
       " (187.24, 187.52),\n",
       " (187.54, 187.94),\n",
       " (187.94, 188.34),\n",
       " (188.34, 188.74),\n",
       " (188.74, 189.14),\n",
       " (189.14, 189.54),\n",
       " (189.54, 189.94),\n",
       " (189.94, 190.34),\n",
       " (190.34, 190.74),\n",
       " (190.74, 190.86),\n",
       " (190.96, 191.36),\n",
       " (191.36, 191.76),\n",
       " (191.76, 192.16),\n",
       " (192.16, 192.56),\n",
       " (192.56, 192.96),\n",
       " (192.96, 193.36),\n",
       " (193.36, 193.76),\n",
       " (193.76, 194.16),\n",
       " (194.16, 194.56),\n",
       " (194.56, 194.96),\n",
       " (194.96, 195.36),\n",
       " (195.36, 195.76),\n",
       " (195.76, 196.16),\n",
       " (196.16, 196.56),\n",
       " (196.56, 196.96),\n",
       " (196.96, 197.36),\n",
       " (197.36, 197.76),\n",
       " (197.76, 198.16),\n",
       " (198.16, 198.56),\n",
       " (198.56, 198.96),\n",
       " (198.96, 199.36),\n",
       " (199.36, 199.76),\n",
       " (199.76, 200.16),\n",
       " (200.16, 200.56),\n",
       " (200.56, 200.96),\n",
       " (200.96, 201.36),\n",
       " (201.36, 201.76),\n",
       " (201.76, 202.16),\n",
       " (202.16, 202.56),\n",
       " (202.56, 202.96),\n",
       " (202.96, 203.36),\n",
       " (203.36, 203.76),\n",
       " (203.76, 204.16),\n",
       " (204.16, 204.56),\n",
       " (204.56, 204.96),\n",
       " (204.96, 205.36),\n",
       " (205.36, 205.76),\n",
       " (205.76, 206.16),\n",
       " (206.16, 206.56),\n",
       " (206.56, 206.96),\n",
       " (206.96, 207.36),\n",
       " (207.36, 207.42),\n",
       " (207.96, 208.36),\n",
       " (208.36, 208.76),\n",
       " (208.76, 209.16),\n",
       " (209.16, 209.56),\n",
       " (209.56, 209.96),\n",
       " (209.96, 210.36),\n",
       " (210.36, 210.76),\n",
       " (210.76, 211.16),\n",
       " (211.16, 211.56),\n",
       " (211.56, 211.96),\n",
       " (211.96, 212.36),\n",
       " (212.36, 212.76),\n",
       " (212.76, 213.16),\n",
       " (213.16, 213.56),\n",
       " (213.56, 213.96),\n",
       " (213.96, 214.36),\n",
       " (214.36, 214.76),\n",
       " (214.76, 215.16),\n",
       " (215.16, 215.56),\n",
       " (215.56, 215.96),\n",
       " (215.96, 216.36),\n",
       " (216.36, 216.76),\n",
       " (216.76, 217.16),\n",
       " (217.16, 217.56),\n",
       " (217.56, 217.96),\n",
       " (217.96, 218.36),\n",
       " (218.36, 218.76),\n",
       " (218.76, 219.16),\n",
       " (219.16, 219.56),\n",
       " (219.56, 219.96),\n",
       " (219.96, 220.36),\n",
       " (220.36, 220.76),\n",
       " (220.76, 221.16),\n",
       " (221.16, 221.56),\n",
       " (221.56, 221.96),\n",
       " (221.96, 222.36),\n",
       " (222.36, 222.76),\n",
       " (222.76, 223.16),\n",
       " (223.16, 223.56),\n",
       " (223.56, 223.96),\n",
       " (223.96, 224.36),\n",
       " (224.36, 224.76),\n",
       " (224.76, 225.16),\n",
       " (225.16, 225.56),\n",
       " (225.56, 225.96),\n",
       " (225.96, 226.36),\n",
       " (226.36, 226.76),\n",
       " (226.76, 227.16),\n",
       " (227.16, 227.56),\n",
       " (227.56, 227.96),\n",
       " (227.96, 228.36),\n",
       " (228.36, 228.76),\n",
       " (228.76, 229.16),\n",
       " (229.16, 229.56),\n",
       " (229.56, 229.96),\n",
       " (229.96, 230.36),\n",
       " (230.36, 230.76),\n",
       " (230.76, 231.16),\n",
       " (231.16, 231.38),\n",
       " (231.4, 231.8),\n",
       " (231.8, 232.2),\n",
       " (232.2, 232.6),\n",
       " (232.6, 233.0),\n",
       " (233.0, 233.4),\n",
       " (233.4, 233.8),\n",
       " (233.8, 234.2),\n",
       " (234.2, 234.6),\n",
       " (234.6, 235.0),\n",
       " (235.0, 235.4),\n",
       " (235.4, 235.8),\n",
       " (235.8, 236.2),\n",
       " (236.2, 236.6),\n",
       " (236.6, 237.0),\n",
       " (237.0, 237.4),\n",
       " (237.4, 237.8),\n",
       " (237.8, 238.2),\n",
       " (238.2, 238.6),\n",
       " (238.6, 239.0),\n",
       " (239.0, 239.4),\n",
       " (239.4, 239.8),\n",
       " (239.8, 240.2),\n",
       " (240.2, 240.6),\n",
       " (240.6, 241.0),\n",
       " (241.0, 241.4),\n",
       " (241.4, 241.8),\n",
       " (241.8, 242.2),\n",
       " (242.2, 242.6),\n",
       " (242.6, 243.0),\n",
       " (243.0, 243.4),\n",
       " (243.4, 243.8),\n",
       " (243.8, 244.2),\n",
       " (244.2, 244.6),\n",
       " (244.6, 245.0),\n",
       " (245.0, 245.4),\n",
       " (245.4, 245.8),\n",
       " (245.8, 245.86),\n",
       " (246.06, 246.46),\n",
       " (246.46, 246.86),\n",
       " (246.86, 247.26),\n",
       " (247.26, 247.66),\n",
       " (247.66, 248.06),\n",
       " (248.06, 248.46),\n",
       " (248.46, 248.86),\n",
       " (248.86, 249.26),\n",
       " (249.26, 249.66),\n",
       " (249.66, 250.06),\n",
       " (250.06, 250.46),\n",
       " (250.46, 250.86),\n",
       " (250.86, 251.26),\n",
       " (251.26, 251.66),\n",
       " (251.66, 252.06),\n",
       " (252.06, 252.46),\n",
       " (252.46, 252.86),\n",
       " (252.86, 253.26),\n",
       " (253.26, 253.66),\n",
       " (253.66, 254.06),\n",
       " (254.06, 254.26),\n",
       " (254.4, 254.8),\n",
       " (254.8, 255.2),\n",
       " (255.2, 255.6),\n",
       " (255.6, 256.0),\n",
       " (256.0, 256.4),\n",
       " (256.4, 256.8),\n",
       " (256.8, 257.2),\n",
       " (257.2, 257.6),\n",
       " (257.6, 258.0),\n",
       " (258.0, 258.4),\n",
       " (258.4, 258.8),\n",
       " (258.8, 259.2),\n",
       " (259.2, 259.6),\n",
       " (259.6, 260.0),\n",
       " (260.0, 260.4),\n",
       " (260.4, 260.8),\n",
       " (260.8, 261.2),\n",
       " (261.2, 261.6),\n",
       " (261.6, 262.0),\n",
       " (262.0, 262.4),\n",
       " (262.4, 262.8),\n",
       " (262.8, 263.2),\n",
       " (263.2, 263.6),\n",
       " (263.6, 264.0),\n",
       " (264.0, 264.4),\n",
       " (264.4, 264.46),\n",
       " (264.58, 264.98),\n",
       " (264.98, 265.38),\n",
       " (265.38, 265.78),\n",
       " (265.78, 266.18),\n",
       " (266.18, 266.58),\n",
       " (266.58, 266.98),\n",
       " (266.98, 267.38),\n",
       " (267.38, 267.78),\n",
       " (267.78, 268.18),\n",
       " (268.18, 268.58),\n",
       " (268.58, 268.98),\n",
       " (268.98, 269.38),\n",
       " (269.38, 269.78),\n",
       " (269.78, 270.18),\n",
       " (270.18, 270.58),\n",
       " (270.58, 270.98),\n",
       " (270.98, 271.38),\n",
       " (271.38, 271.78),\n",
       " (271.78, 272.18),\n",
       " (272.18, 272.58),\n",
       " (272.58, 272.98),\n",
       " (272.98, 273.38),\n",
       " (273.38, 273.78),\n",
       " (273.78, 274.18),\n",
       " (274.18, 274.58),\n",
       " (274.58, 274.98),\n",
       " (274.98, 275.38),\n",
       " (275.38, 275.78),\n",
       " (275.78, 276.18),\n",
       " (276.18, 276.58),\n",
       " (276.58, 276.98),\n",
       " (276.98, 277.38),\n",
       " (277.38, 277.78),\n",
       " (277.78, 278.18),\n",
       " (278.18, 278.58),\n",
       " (278.58, 278.98),\n",
       " (278.98, 279.38),\n",
       " (279.38, 279.78),\n",
       " (279.78, 280.18),\n",
       " (280.18, 280.24)]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(embeddings, intervals):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    \n",
    "    avg_embeddings = np.zeros((len(partitions),256))\n",
    "    segment_intervals = [] \n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0)\n",
    "        \n",
    "        partition_interval = intervals[partition[0]:partition[1]]\n",
    "        interval_onset = partition_interval[0][0]   #start of first partition\n",
    "        interval_offset = partition_interval[-2][1] #end of last partition\n",
    "        segment_intervals.append([interval_onset, interval_offset])\n",
    "    return avg_embeddings, np.array(segment_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, config.model_path)\n",
    "   \n",
    "    audio_count = 0\n",
    "    train_sequence = np.array([]).reshape(0,256)\n",
    "    train_cluster_ids = []\n",
    "    \n",
    "    for audio_id, audio_path in audio_files.items():\n",
    "        logging.info(f'loading {audio_id} {audio_count}/{audio_quantity}')\n",
    "\n",
    "        # voice activity detection            \n",
    "        times, segs = VAD_chunk(2, audio_path)\n",
    "        concat_seg = concat_segs(times, segs)\n",
    "        STFT_windows, time = get_STFTs(concat_seg)\n",
    "        # print(len(STFT_windows), STFT_windows[0].shape)\n",
    "\n",
    "        embeddings = np.array([]).reshape(0,256)\n",
    "        for idx, STFT_window in enumerate(STFT_windows):\n",
    "            STFT_batch = np.transpose(STFT_window, axes=(2,0,1))\n",
    "            # print(STFT_batch.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\n",
    "            embeddings_batch = sess.run(embedded, feed_dict={verif:STFT_batch})\n",
    "            embeddings = np.concatenate((embeddings, embeddings_batch))\n",
    "            \n",
    "        aligned_embeddings = align_embeddings(embeddings) # Turn window-level embeddings to segment-level (400ms)\n",
    "        \n",
    "        train_sequence = np.concatenate((train_sequence, aligned_embeddings))\n",
    "\n",
    "        # Precisa obter o speaker a partir de cada intervalo de 0.4s definido no aligned_embeddings\n",
    "        # Levar em consideração o 'times' retornado pelo VAD\n",
    "        # Comparar com o turns retornado pelo load_rttm\n",
    "        for embedding in aligned_embeddings:\n",
    "            train_cluster_ids.append(str(speaker_count))\n",
    "\n",
    "        audio_count += 1\n",
    "                    \n",
    "    # Verificar se não estamos concatenando tudo em uma sequencia só. Pode atrapalhar no treinamento\n",
    "    train_sequence_path = os.path.join(save_dir_path, f'voxcon-dev-train-sequence.npy')\n",
    "    np.save(train_sequence_path, train_sequence)\n",
    "            \n",
    "    train_cluster_ids_path = os.path.join(save_dir_path, f'voxcon-dev-train-cluster-ids.npy')\n",
    "    train_cluster_ids = np.asarray(train_cluster_ids)\n",
    "    np.save(train_cluster_ids_path, train_cluster_ids)\n",
    "    logging.info(f'saved train sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = []\n",
    "train_sequence.append(aligned_embeddings)\n",
    "print(len(train_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_STFTs(segs):\n",
    "#     #Get 240ms STFT windows with 50% overlap\n",
    "#     sr = config.sr\n",
    "#     STFT_frames = []\n",
    "#     for seg in segs:\n",
    "#         S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "#         S = np.abs(S) ** 2\n",
    "#         mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "#         # log mel spectrogram of utterances\n",
    "#         S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "#         for j in range(0, S.shape[1], int(.12/config.hop)):\n",
    "#             if j + 24 < S.shape[1]:\n",
    "#                 STFT_frames.append(S[:,j:j+24])\n",
    "#             else:\n",
    "#                 break\n",
    "#     return STFT_frames\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from hparam import hparam as hp\n",
    "\n",
    "# class SpeechEmbedder(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SpeechEmbedder, self).__init__()    \n",
    "#         self.LSTM_stack = nn.LSTM(hp.data.nmels, hp.model.hidden, num_layers=hp.model.num_layer, batch_first=True)\n",
    "#         for name, param in self.LSTM_stack.named_parameters():\n",
    "#           if 'bias' in name:\n",
    "#              nn.init.constant_(param, 0.0)\n",
    "#           elif 'weight' in name:\n",
    "#              nn.init.xavier_normal_(param)\n",
    "#         self.projection = nn.Linear(hp.model.hidden, hp.model.proj)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n",
    "#         #only use last frame\n",
    "#         x = x[:,x.size(1)-1]\n",
    "#         x = self.projection(x.float())\n",
    "#         x = x / torch.norm(x, dim=1).unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "# embedder_net = SpeechEmbedder()\n",
    "# # embedder_net.load_state_dict(torch.load(hp.model.model_path))\n",
    "# embedder_net.eval()\n",
    "\n",
    "# times, segs = VAD_chunk(2, audio_path)\n",
    "# concat_seg = concat_segs(times, segs)\n",
    "# STFT_frames = get_STFTs(concat_seg)\n",
    "# STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "# STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "\n",
    "# embeddings = embedder_net(STFT_frames)\n",
    "\n",
    "# embeddings.shape\n",
    "\n",
    "# STFT_frames.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
