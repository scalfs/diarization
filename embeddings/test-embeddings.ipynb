{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(audio_file='../data/voxconverse/sample/abjxc.wav', hidden=768, hop=0.01, log_path='main.logs', model_num=3, model_path='../models/model.ckpt-46', nfft=512, num_layer=3, number_of_speakers=2, output_dir='output.json', proj=256, random_state=123, restore=False, sr=16000, srt_path='xxx.en.srt', tdsv=False, tdsv_frame=160, tisv_frame=160, tisv_frame_min=50, window=0.025)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from utils import normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from collections import defaultdict\n",
    "from configuration import get_config\n",
    "from rttm import load_rttm\n",
    "from VAD_segments import VAD_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: /home/jovyan/work/voxsrc21-dia/embeddings/voxconverse-sample-embeddings.logs\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config.log_path = 'voxconverse-sample-embeddings.logs'\n",
    "log_file = os.path.abspath(config.log_path)\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-092a808a835d>:15: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-092a808a835d>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-092a808a835d>:17: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jovyan/work/voxsrc21-dia/embeddings/utils.py:8: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/wav'\n",
    "rttm_path = '/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/rttm'\n",
    "save_dir_path = '/home/jovyan/work/voxsrc21-dia/embeddings/sequences/voxconverse-sample'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Data prep\n",
    "# I'm saving only 2 embeddings i.e. first and last tisv_frames for given interval in an audio. So each .npy\n",
    "# embedding file will have a shape of (2, 256)\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2 # Fixing to 2 since we take 2 for each interval #utter_batch.shape[1]\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "config_tensorflow = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_segs(times, segs):\n",
    "    #Concatenate continuous voiced segments\n",
    "    concat_seg = []\n",
    "    seg_concat = segs[0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "    return concat_seg\n",
    "\n",
    "def align_embeddings(embeddings):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    avg_embeddings = np.zeros((len(partitions),256))\n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0) \n",
    "    return avg_embeddings\n",
    "\n",
    "def get_STFTs(segs):\n",
    "    #Get 240ms STFT windows with 50% overlap, in pairs\n",
    "    sr = config.sr\n",
    "    STFT_windows = []\n",
    "    for seg in segs:\n",
    "        S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "        # log mel spectrogram of utterances\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "        for j in range(0, S.shape[1], int(.24/config.hop)):\n",
    "            if j + 36 < S.shape[1]:\n",
    "                # in order to fit on the expected shape of the embedding network we double the window\n",
    "                STFT_windows.append([S[:, j:j+24], S[:, j+12:j+36]])                \n",
    "            else:\n",
    "                break\n",
    "    return np.array(STFT_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique file extensions: {'.wav'}\n",
      "Number of audios: 19\n",
      "Number of rttms: 19\n"
     ]
    }
   ],
   "source": [
    "all_unique_extensions = []\n",
    "# Using List as default factory\n",
    "audio_files = defaultdict(list)\n",
    "rttm_files = defaultdict(list)\n",
    "\n",
    "for audio_file in os.listdir(data_path):\n",
    "    if audio_file.startswith('.'): #hidden folders\n",
    "        continue;\n",
    "    audio_id = os.path.splitext(audio_file)[0]\n",
    "    extension = os.path.splitext(audio_file)[1]\n",
    "    all_unique_extensions.append(extension)\n",
    "#     print(f'Audio id: {audio_id}')\n",
    "    if extension == '.wav':\n",
    "        audio_files[audio_id].append(os.path.join(data_path, audio_file))\n",
    "        rttm_files[audio_id].append(os.path.join(rttm_path, audio_id + '.rttm'))\n",
    "    else:\n",
    "        print(f'Wrong file type in {os.path.join(data_path, audio_file)}')\n",
    "\n",
    "audio_quantity = len(audio_files)\n",
    "print(f'Unique file extensions: {set(all_unique_extensions)}')\n",
    "print(f'Number of audios: {audio_quantity}')\n",
    "print(f'Number of rttms: {len(rttm_files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/voxsrc21-dia/data/voxconverse/sample/rttm/abjxc.rttm'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rttm_files.get('abjxc')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Turn(0.400000, 7.040000, None, 'spk00', 'abjxc'),\n",
       " Turn(8.680000, 64.640000, None, 'spk00', 'abjxc')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "turns, _, _ = load_rttm(rttm_files.get('abjxc')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, config.model_path)\n",
    "   \n",
    "    audio_count = 0\n",
    "    train_sequence = np.array([]).reshape(0,256)\n",
    "    train_cluster_ids = []\n",
    "    \n",
    "    for audio_id, audio_path in audio_files.items():\n",
    "        video_id = audio_path.split('/')[-2]\n",
    "        audio_id = audio_path.split('/')[-1].replace('.wav','')\n",
    "            \n",
    "        logging.info(f'loading {audio_id} {audio_count}/{audio_quantity}')\n",
    "\n",
    "        # voice activity detection            \n",
    "        times, segs = VAD_chunk(2, audio_path)\n",
    "        concat_seg = concat_segs(times, segs)\n",
    "        STFT_windows = get_STFTs(concat_seg)\n",
    "        # print(len(STFT_windows), STFT_windows[0].shape)\n",
    "\n",
    "        embeddings = np.array([]).reshape(0,256)\n",
    "        for STFT_window in STFT_windows:\n",
    "            STFT_batch = np.transpose(STFT_window, axes=(2,0,1))\n",
    "            # print(STFT_frames2.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\n",
    "            embeddings_batch = sess.run(embedded, feed_dict={verif:STFT_batch})\n",
    "            embeddings = np.concatenate((embeddings, embeddings_batch))\n",
    "            \n",
    "        print(len(STFT_windows))\n",
    "        print(len(embeddings))\n",
    "        aligned_embeddings = align_embeddings(embeddings) # Turn window-level embeddings to segment-level (400ms)\n",
    "        print(len(aligned_embeddings))\n",
    "            \n",
    "        train_sequence = np.concatenate((train_sequence, aligned_embeddings))\n",
    "\n",
    "        # Precisa obter o speaker a partir de cada intervalo de 0.4s definido no aligned_embeddings\n",
    "        # Levar em consideração o 'times' retornado pelo VAD\n",
    "        # Comparar com o turns retornado pelo load_rttm\n",
    "        for embedding in aligned_embeddings:\n",
    "            train_cluster_ids.append(str(speaker_count))\n",
    "\n",
    "        audio_count += 1\n",
    "                    \n",
    "    # Verificar se não estamos concatenando tudo em uma sequencia só. Pode atrapalhar no treinamento\n",
    "    train_sequence_path = os.path.join(save_dir_path, f'voxcon-dev-train-sequence.npy')\n",
    "    np.save(train_sequence_path, train_sequence)\n",
    "            \n",
    "    train_cluster_ids_path = os.path.join(save_dir_path, f'voxcon-dev-train-cluster-ids.npy')\n",
    "    train_cluster_ids = np.asarray(train_cluster_ids)\n",
    "    np.save(train_cluster_ids_path, train_cluster_ids)\n",
    "    logging.info(f'saved train sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = []\n",
    "train_sequence.append(aligned_embeddings)\n",
    "print(len(train_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = librosa.effects.split(utter, top_db=vad_threshold)\n",
    "for idx, current_interval in enumerate(intervals):\n",
    "    utterances_spec = []\n",
    "    utter_part = utter[current_interval[0]:current_interval[1]]\n",
    "    S = librosa.core.stft(y=utter_part, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "    S = np.abs(S) ** 2\n",
    "    mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "    # log mel spectrogram of utterances\n",
    "    S = np.log10(np.dot(mel_basis, S) + 1e-6)\n",
    "    \n",
    "    utterances_spec.append(S[:, :config.tisv_frame])\n",
    "    utterances_spec.append(S[:, -config.tisv_frame:])\n",
    "    \n",
    "    utterances_spec = np.array(utterances_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     \"\"\"\n",
    "#     Speaker embeddings program:\n",
    "#     input: audio files\n",
    "#     output: npy file with shape (2, 256) [first and last tisv_frames for given interval in an audio]\n",
    "#     \"\"\"\n",
    "#     main()\n",
    "#     print('Program completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_STFTs(segs):\n",
    "#     #Get 240ms STFT windows with 50% overlap\n",
    "#     sr = config.sr\n",
    "#     STFT_frames = []\n",
    "#     for seg in segs:\n",
    "#         S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "#         S = np.abs(S) ** 2\n",
    "#         mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "#         # log mel spectrogram of utterances\n",
    "#         S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "#         for j in range(0, S.shape[1], int(.12/config.hop)):\n",
    "#             if j + 24 < S.shape[1]:\n",
    "#                 STFT_frames.append(S[:,j:j+24])\n",
    "#             else:\n",
    "#                 break\n",
    "#     return STFT_frames\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from hparam import hparam as hp\n",
    "\n",
    "# class SpeechEmbedder(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SpeechEmbedder, self).__init__()    \n",
    "#         self.LSTM_stack = nn.LSTM(hp.data.nmels, hp.model.hidden, num_layers=hp.model.num_layer, batch_first=True)\n",
    "#         for name, param in self.LSTM_stack.named_parameters():\n",
    "#           if 'bias' in name:\n",
    "#              nn.init.constant_(param, 0.0)\n",
    "#           elif 'weight' in name:\n",
    "#              nn.init.xavier_normal_(param)\n",
    "#         self.projection = nn.Linear(hp.model.hidden, hp.model.proj)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n",
    "#         #only use last frame\n",
    "#         x = x[:,x.size(1)-1]\n",
    "#         x = self.projection(x.float())\n",
    "#         x = x / torch.norm(x, dim=1).unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "# embedder_net = SpeechEmbedder()\n",
    "# # embedder_net.load_state_dict(torch.load(hp.model.model_path))\n",
    "# embedder_net.eval()\n",
    "\n",
    "# times, segs = VAD_chunk(2, audio_path)\n",
    "# concat_seg = concat_segs(times, segs)\n",
    "# STFT_frames = get_STFTs(concat_seg)\n",
    "# STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "# STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "\n",
    "# embeddings = embedder_net(STFT_frames)\n",
    "\n",
    "# embeddings.shape\n",
    "\n",
    "# STFT_frames.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
