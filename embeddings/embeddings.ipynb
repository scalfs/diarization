{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from utils import normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from collections import defaultdict\n",
    "from configuration import get_config\n",
    "\n",
    "from VAD_segments import VAD_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: /home/jovyan/work/voxsrc21-dia/embeddings/main.logs\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "log_file = os.path.abspath(config.log_path)\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing to 25, which will give slightly better intervals, 20 gives very short intervals\n",
    "vad_threshold = 25 # threshold for voice activity detection\n",
    "\n",
    "data_path = '/home/jovyan/work/datasets/voxceleb-1/sample/wav'\n",
    "save_dir_path = '/home/jovyan/work/voxsrc21-dia/embeddings/sequences'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Data prep\n",
    "# I'm saving only 2 embeddings i.e. first and last tisv_frames for given interval in an audio. So each .npy\n",
    "# embedding file will have a shape of (2, 256)\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2 # Fixing to 2 since we take 2 for each interval #utter_batch.shape[1]\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "config_tensorflow = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique file extensions: {'.wav'}\n",
      "Number of speakers: 19\n",
      "Number of audios: 2198\n"
     ]
    }
   ],
   "source": [
    "all_unique_extensions = []\n",
    "all_files = defaultdict(list)\n",
    "audio_quantity = 0\n",
    "for base_id in os.listdir(data_path):\n",
    "#     print(f'Base id: {base_id}')\n",
    "    if base_id.startswith('.'): #hidden folders\n",
    "        continue;\n",
    "    for video_id in os.listdir(os.path.join(data_path, base_id)):\n",
    "#         print(f'Base id: {base_id} Video id: {video_id}')\n",
    "        if video_id.startswith('.'): #hidden folders\n",
    "            continue;\n",
    "        for audio_id in os.listdir(os.path.join(data_path, base_id, video_id)):\n",
    "#             print(f'Base id: {base_id} Video id: {video_id} Audio id: {audio_id}')\n",
    "            all_unique_extensions.append(os.path.splitext(audio_id)[1])\n",
    "            if os.path.splitext(audio_id)[1] == '.wav':\n",
    "                # append the file path and save path to all_files\n",
    "                all_files[base_id].append(os.path.join(data_path, base_id, video_id, audio_id))\n",
    "                audio_quantity += 1\n",
    "            else:\n",
    "                print(f'Wrong file type in {os.path.join(data_path, base_id, video_id, audio_id)}')\n",
    "print(f'Unique file extensions: {set(all_unique_extensions)}')\n",
    "print(f'Number of speakers: {len(all_files)}')\n",
    "print(f'Number of audios: {audio_quantity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_segs(times, segs):\n",
    "    #Concatenate continuous voiced segments\n",
    "    concat_seg = []\n",
    "    seg_concat = segs[0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "    return concat_seg\n",
    "\n",
    "def align_embeddings(embeddings):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    avg_embeddings = np.zeros((len(partitions),256))\n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0) \n",
    "    return avg_embeddings\n",
    "\n",
    "def get_STFTs(segs):\n",
    "    #Get 240ms STFT windows with 50% overlap\n",
    "    sr = config.sr\n",
    "    STFT_windows = []\n",
    "    for seg in segs:\n",
    "        S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "        # log mel spectrogram of utterances\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "        for j in range(0, S.shape[1], int(.12/config.hop)):\n",
    "            if j + 48 < S.shape[1]:\n",
    "                # in order to fit on the expected shape of the embedding network we double the window\n",
    "                STFT_windows.append([S[:, j:j+24], S[:, j+24:j+48]])                \n",
    "            else:\n",
    "                break\n",
    "    return np.array(STFT_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 43, 256)\n"
     ]
    }
   ],
   "source": [
    "## AACCEEERRTTAAARRRR\n",
    "\n",
    "a = np.load(train_sequences_path)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/model.ckpt-46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-556e5f7a4302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mSTFT_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTFT_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# print(STFT_frames2.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0membeddings_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mverif\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mSTFT_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, config.model_path)\n",
    "    \n",
    "    speaker_count = 0\n",
    "    total_speakers = len(all_files)\n",
    "    speakers_per_batch = 50 # config.N\n",
    "    \n",
    "    speaker_label = 0\n",
    "    train_sequences = np.array([]).reshape(0,256)\n",
    "    train_cluster_ids = []\n",
    "    \n",
    "    for speaker_id, audio_paths in all_files.items():\n",
    "        for audio_path in audio_paths:\n",
    "            video_id = audio_path.split('/')[-2]\n",
    "            audio_id = audio_path.split('/')[-1].replace('.wav','')\n",
    "            audio_count += 1\n",
    "            \n",
    "            logging.info(f'loading {speaker_id}-{video_id}-{audio_id} {audio_count}/{audio_quantity}')\n",
    "            utter, sr = librosa.core.load(audio_path, sr=config.sr)\n",
    "            # lower bound of utterance length\n",
    "            # utter_min_len = (config.tisv_frame_min * config.hop + config.window) * sr\n",
    "            # Get the duration\n",
    "            # duration = librosa.get_duration(utter, sr)\n",
    "            # Duration of each window\n",
    "            # duration_per_frame = (duration / utter.shape[0])\n",
    "            # logging.info(f'Duration: {duration}\\nDuration per frame: {duration_per_frame}s\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "            # tisv_frame_duration_s = utter_min_len * duration_per_frame\n",
    "            \n",
    "            # voice activity detection            \n",
    "            times, segs = VAD_chunk(2, audio_path)\n",
    "            concat_seg = concat_segs(times, segs)\n",
    "            STFT_windows = get_STFTs(concat_seg)\n",
    "            # print(len(STFT_windows), STFT_windows[0].shape)\n",
    "\n",
    "            embeddings = np.array([]).reshape(0,256)\n",
    "            for STFT_window in STFT_windows:\n",
    "                STFT_batch = np.transpose(STFT_window, axes=(2,0,1))\n",
    "                # print(STFT_frames2.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\n",
    "                embeddings_batch = sess.run(embedded, feed_dict={verif:STFT_batch})\n",
    "                embeddings = np.concatenate((embeddings, embeddings_batch))\n",
    "    \n",
    "            aligned_embeddings = align_embeddings(embeddings) # Turn window-level embeddings to segment-level (400ms)\n",
    "            \n",
    "            train_sequences = np.concatenate((train_sequences, aligned_embeddings))\n",
    "            for embedding in aligned_embeddings:\n",
    "                train_cluster_ids.append(str(speaker_label))\n",
    "        \n",
    "        speaker_count += 1\n",
    "        speaker_label += 1\n",
    "        if (speaker_count != total_speakers or speaker_count % speakers_per_batch == 0):\n",
    "            train_sequences_path = os.path.join(save_dir_path, f'vox1-train-sequences-{speaker_count}.npy')\n",
    "            np.save(train_sequences_path, train_sequence)\n",
    "            \n",
    "            train_cluster_ids_path = os.path.join(save_dir_path, f'vox1-train-cluster-ids-{speaker_count}.npy')\n",
    "            train_cluster_ids = np.asarray(train_cluster_ids)\n",
    "            np.save(train_cluster_ids_path, train_cluster_ids)\n",
    "            logging.info(f'saved batch {speaker_count/total_speakers}/{speakers_per_batch/total_speakers}')\n",
    "            \n",
    "            train_sequences = np.array([]).reshape(0,256)\n",
    "            train_cluster_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "train_sequence = []\n",
    "train_sequence.append(aligned_embeddings)\n",
    "print(len(train_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 40, 36)\n",
      "(2, 40, 160)\n"
     ]
    }
   ],
   "source": [
    "intervals = librosa.effects.split(utter, top_db=vad_threshold)\n",
    "for idx, current_interval in enumerate(intervals):\n",
    "    utterances_spec = []\n",
    "    utter_part = utter[current_interval[0]:current_interval[1]]\n",
    "    S = librosa.core.stft(y=utter_part, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "    S = np.abs(S) ** 2\n",
    "    mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "    # log mel spectrogram of utterances\n",
    "    S = np.log10(np.dot(mel_basis, S) + 1e-6)\n",
    "    \n",
    "    utterances_spec.append(S[:, :config.tisv_frame])\n",
    "    utterances_spec.append(S[:, -config.tisv_frame:])\n",
    "    \n",
    "    utterances_spec = np.array(utterances_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     \"\"\"\n",
    "#     Speaker embeddings program:\n",
    "#     input: audio files\n",
    "#     output: npy file with shape (2, 256) [first and last tisv_frames for given interval in an audio]\n",
    "#     \"\"\"\n",
    "#     main()\n",
    "#     print('Program completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_STFTs(segs):\n",
    "#     #Get 240ms STFT windows with 50% overlap\n",
    "#     sr = config.sr\n",
    "#     STFT_frames = []\n",
    "#     for seg in segs:\n",
    "#         S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "#         S = np.abs(S) ** 2\n",
    "#         mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "#         # log mel spectrogram of utterances\n",
    "#         S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "#         for j in range(0, S.shape[1], int(.12/config.hop)):\n",
    "#             if j + 24 < S.shape[1]:\n",
    "#                 STFT_frames.append(S[:,j:j+24])\n",
    "#             else:\n",
    "#                 break\n",
    "#     return STFT_frames\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from hparam import hparam as hp\n",
    "\n",
    "# class SpeechEmbedder(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SpeechEmbedder, self).__init__()    \n",
    "#         self.LSTM_stack = nn.LSTM(hp.data.nmels, hp.model.hidden, num_layers=hp.model.num_layer, batch_first=True)\n",
    "#         for name, param in self.LSTM_stack.named_parameters():\n",
    "#           if 'bias' in name:\n",
    "#              nn.init.constant_(param, 0.0)\n",
    "#           elif 'weight' in name:\n",
    "#              nn.init.xavier_normal_(param)\n",
    "#         self.projection = nn.Linear(hp.model.hidden, hp.model.proj)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n",
    "#         #only use last frame\n",
    "#         x = x[:,x.size(1)-1]\n",
    "#         x = self.projection(x.float())\n",
    "#         x = x / torch.norm(x, dim=1).unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "# embedder_net = SpeechEmbedder()\n",
    "# # embedder_net.load_state_dict(torch.load(hp.model.model_path))\n",
    "# embedder_net.eval()\n",
    "\n",
    "# times, segs = VAD_chunk(2, audio_path)\n",
    "# concat_seg = concat_segs(times, segs)\n",
    "# STFT_frames = get_STFTs(concat_seg)\n",
    "# STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "# STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "\n",
    "# embeddings = embedder_net(STFT_frames)\n",
    "\n",
    "# embeddings.shape\n",
    "\n",
    "# STFT_frames.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
