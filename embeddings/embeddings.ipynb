{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(audio_file='../data/voxconverse/sample/abjxc.wav', hidden=768, hop=0.01, log_path='main.logs', model_num=3, model_path='../models/model.ckpt-46', nfft=512, num_layer=3, number_of_speakers=2, output_dir='output.json', proj=256, random_state=123, restore=False, sr=16000, srt_path='xxx.en.srt', tdsv=False, tdsv_frame=160, tisv_frame=160, tisv_frame_min=50, window=0.025)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import librosa\n",
    "import numpy as np\n",
    "from utils import normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "from collections import defaultdict\n",
    "from configuration import get_config\n",
    "\n",
    "from VAD_segments import VAD_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log path: /home/jovyan/work/voxsrc21-dia/embeddings/main.logs\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "config.log_path = 'voxceleb1-embeddings.logs'\n",
    "log_file = os.path.abspath(config.log_path)\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    "    )\n",
    "print(f'Log path: {log_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-51945b692c05>:17: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-51945b692c05>:18: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-3-51945b692c05>:19: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jovyan/work/voxsrc21-dia/embeddings/utils.py:8: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Changing to 25, which will give slightly better intervals, 20 gives very short intervals\n",
    "vad_threshold = 25 # threshold for voice activity detection\n",
    "\n",
    "data_path = '/home/jovyan/work/datasets/voxceleb-1/sample/wav'\n",
    "save_dir_path = '/home/jovyan/work/voxsrc21-dia/embeddings/sequences'\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "# Data prep\n",
    "# I'm saving only 2 embeddings i.e. first and last tisv_frames for given interval in an audio. So each .npy\n",
    "# embedding file will have a shape of (2, 256)\n",
    "tf.reset_default_graph()\n",
    "batch_size = 2 # Fixing to 2 since we take 2 for each interval #utter_batch.shape[1]\n",
    "verif = tf.placeholder(shape=[None, batch_size, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n",
    "batch = tf.concat([verif,], axis=1)\n",
    "# embedding lstm (3-layer default)\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n",
    "    lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n",
    "    outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n",
    "    embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n",
    "    embedded = normalize(embedded)                    # normalize\n",
    "config_tensorflow = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "saver = tf.train.Saver(var_list=tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique file extensions: {'.wav'}\n",
      "Number of speakers: 19\n",
      "Number of audios: 2198\n"
     ]
    }
   ],
   "source": [
    "all_unique_extensions = []\n",
    "all_files = defaultdict(list)\n",
    "audio_quantity = 0\n",
    "for base_id in os.listdir(data_path):\n",
    "#     print(f'Base id: {base_id}')\n",
    "    if base_id.startswith('.'): #hidden folders\n",
    "        continue;\n",
    "    for video_id in os.listdir(os.path.join(data_path, base_id)):\n",
    "#         print(f'Base id: {base_id} Video id: {video_id}')\n",
    "        if video_id.startswith('.'): #hidden folders\n",
    "            continue;\n",
    "        for audio_id in os.listdir(os.path.join(data_path, base_id, video_id)):\n",
    "#             print(f'Base id: {base_id} Video id: {video_id} Audio id: {audio_id}')\n",
    "            all_unique_extensions.append(os.path.splitext(audio_id)[1])\n",
    "            if os.path.splitext(audio_id)[1] == '.wav':\n",
    "                # append the file path and save path to all_files\n",
    "                all_files[base_id].append(os.path.join(data_path, base_id, video_id, audio_id))\n",
    "                audio_quantity += 1\n",
    "            else:\n",
    "                print(f'Wrong file type in {os.path.join(data_path, base_id, video_id, audio_id)}')\n",
    "print(f'Unique file extensions: {set(all_unique_extensions)}')\n",
    "print(f'Number of speakers: {len(all_files)}')\n",
    "print(f'Number of audios: {audio_quantity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_segs(times, segs):\n",
    "    #Concatenate continuous voiced segments\n",
    "    concat_seg = []\n",
    "    seg_concat = segs[0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "    return concat_seg\n",
    "\n",
    "def align_embeddings(embeddings):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    avg_embeddings = np.zeros((len(partitions),256))\n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0) \n",
    "    return avg_embeddings\n",
    "\n",
    "def get_STFTs(segs):\n",
    "    #Get 240ms STFT windows with 50% overlap, in pairs\n",
    "    sr = config.sr\n",
    "    STFT_windows = []\n",
    "    for seg in segs:\n",
    "        S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "        S = np.abs(S) ** 2\n",
    "        mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "        # log mel spectrogram of utterances\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "        for j in range(0, S.shape[1], int(.24/config.hop)):\n",
    "            if j + 36 < S.shape[1]:\n",
    "                # in order to fit on the expected shape of the embedding network we double the window\n",
    "                STFT_windows.append([S[:, j:j+24], S[:, j+12:j+36]])                \n",
    "            else:\n",
    "                break\n",
    "    return np.array(STFT_windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/model.ckpt-46\n",
      "1 False False\n",
      "False\n",
      "2 False False\n",
      "False\n",
      "3 False False\n",
      "False\n",
      "4 False False\n",
      "False\n",
      "5 False True\n",
      "True\n",
      "saved batch 1/4\n",
      "6 False False\n",
      "False\n",
      "7 False False\n",
      "False\n",
      "8 False False\n",
      "False\n",
      "9 False False\n",
      "False\n",
      "10 False True\n",
      "True\n",
      "saved batch 2/4\n",
      "11 False False\n",
      "False\n",
      "12 False False\n",
      "False\n",
      "13 False False\n",
      "False\n",
      "14 False False\n",
      "False\n",
      "15 False True\n",
      "True\n",
      "saved batch 3/4\n",
      "16 False False\n",
      "False\n",
      "17 False False\n",
      "False\n",
      "18 False False\n",
      "False\n",
      "19 True False\n",
      "True\n",
      "saved batch 4/4\n"
     ]
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "# Each embedding saved file will have (2, 256)\n",
    "with tf.Session(config=config_tensorflow) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver.restore(sess, config.model_path)\n",
    "    \n",
    "    speaker_count = 0\n",
    "    total_speakers = len(all_files)\n",
    "    speakers_per_batch = 50 # config.N\n",
    "    \n",
    "    batch_count = 0\n",
    "    audio_count = 0\n",
    "    train_sequence = np.array([]).reshape(0,256)\n",
    "    train_cluster_ids = []\n",
    "    \n",
    "    for speaker_id, audio_paths in all_files.items():\n",
    "        for audio_path in audio_paths:\n",
    "            video_id = audio_path.split('/')[-2]\n",
    "            audio_id = audio_path.split('/')[-1].replace('.wav','')\n",
    "            \n",
    "            logging.info(f'loading {speaker_id}-{video_id}-{audio_id} {audio_count}/{audio_quantity}')\n",
    "            utter, sr = librosa.core.load(audio_path, sr=config.sr)\n",
    "            # lower bound of utterance length\n",
    "            # utter_min_len = (config.tisv_frame_min * config.hop + config.window) * sr\n",
    "            # Get the duration\n",
    "            # duration = librosa.get_duration(utter, sr)\n",
    "            # Duration of each window\n",
    "            # duration_per_frame = (duration / utter.shape[0])\n",
    "            # logging.info(f'Duration: {duration}\\nDuration per frame: {duration_per_frame}s\\nMin length of utterance: {utter_min_len * duration_per_frame}s')\n",
    "            # tisv_frame_duration_s = utter_min_len * duration_per_frame\n",
    "            \n",
    "            # voice activity detection            \n",
    "            times, segs = VAD_chunk(2, audio_path)\n",
    "            concat_seg = concat_segs(times, segs)\n",
    "            STFT_windows = get_STFTs(concat_seg)\n",
    "            # print(len(STFT_windows), STFT_windows[0].shape)\n",
    "\n",
    "            embeddings = np.array([]).reshape(0,256)\n",
    "            for STFT_window in STFT_windows:\n",
    "                STFT_batch = np.transpose(STFT_window, axes=(2,0,1))\n",
    "                # print(STFT_frames2.shape) (24, 2, 40) (240ms window * batch 2 * mels 40)\n",
    "                embeddings_batch = sess.run(embedded, feed_dict={verif:STFT_batch})\n",
    "                embeddings = np.concatenate((embeddings, embeddings_batch))\n",
    "            \n",
    "            print(len(STFT_windows))\n",
    "            \n",
    "            print(len(embeddings))\n",
    "\n",
    "            aligned_embeddings = align_embeddings(embeddings) # Turn window-level embeddings to segment-level (400ms)\n",
    "            \n",
    "            print(len(aligned_embeddings))\n",
    "            \n",
    "            train_sequence = np.concatenate((train_sequence, aligned_embeddings))\n",
    "            for embedding in aligned_embeddings:\n",
    "                train_cluster_ids.append(str(speaker_count))\n",
    "            \n",
    "            audio_count += 1\n",
    "            \n",
    "        speaker_count += 1\n",
    "        \n",
    "        if (speaker_count == total_speakers or speaker_count % speakers_per_batch == 0):\n",
    "            train_sequence_path = os.path.join(save_dir_path, f'vox1-train-sequences-{batch_count}.npy')\n",
    "            np.save(train_sequence_path, train_sequence)\n",
    "            \n",
    "            train_cluster_ids_path = os.path.join(save_dir_path, f'vox1-train-cluster-ids-{batch_count}.npy')\n",
    "            train_cluster_ids = np.asarray(train_cluster_ids)\n",
    "            np.save(train_cluster_ids_path, train_cluster_ids)\n",
    "            logging.info(f'saved batch {batch_count+1}/{math.ceil(total_speakers/speakers_per_batch)}')\n",
    "            \n",
    "            batch_count += 1\n",
    "            train_sequence = np.array([]).reshape(0,256)\n",
    "            train_cluster_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "train_sequence = []\n",
    "train_sequence.append(aligned_embeddings)\n",
    "print(len(train_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 40, 36)\n",
      "(2, 40, 160)\n"
     ]
    }
   ],
   "source": [
    "intervals = librosa.effects.split(utter, top_db=vad_threshold)\n",
    "for idx, current_interval in enumerate(intervals):\n",
    "    utterances_spec = []\n",
    "    utter_part = utter[current_interval[0]:current_interval[1]]\n",
    "    S = librosa.core.stft(y=utter_part, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "    S = np.abs(S) ** 2\n",
    "    mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "    # log mel spectrogram of utterances\n",
    "    S = np.log10(np.dot(mel_basis, S) + 1e-6)\n",
    "    \n",
    "    utterances_spec.append(S[:, :config.tisv_frame])\n",
    "    utterances_spec.append(S[:, -config.tisv_frame:])\n",
    "    \n",
    "    utterances_spec = np.array(utterances_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     \"\"\"\n",
    "#     Speaker embeddings program:\n",
    "#     input: audio files\n",
    "#     output: npy file with shape (2, 256) [first and last tisv_frames for given interval in an audio]\n",
    "#     \"\"\"\n",
    "#     main()\n",
    "#     print('Program completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_STFTs(segs):\n",
    "#     #Get 240ms STFT windows with 50% overlap\n",
    "#     sr = config.sr\n",
    "#     STFT_frames = []\n",
    "#     for seg in segs:\n",
    "#         S = librosa.core.stft(y=seg, n_fft=config.nfft, win_length=int(config.window * sr), hop_length=int(config.hop * sr))\n",
    "#         S = np.abs(S) ** 2\n",
    "#         mel_basis = librosa.filters.mel(sr=sr, n_fft=config.nfft, n_mels=40)\n",
    "#         # log mel spectrogram of utterances\n",
    "#         S = np.log10(np.dot(mel_basis, S) + 1e-6)        \n",
    "#         for j in range(0, S.shape[1], int(.12/config.hop)):\n",
    "#             if j + 24 < S.shape[1]:\n",
    "#                 STFT_frames.append(S[:,j:j+24])\n",
    "#             else:\n",
    "#                 break\n",
    "#     return STFT_frames\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from hparam import hparam as hp\n",
    "\n",
    "# class SpeechEmbedder(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(SpeechEmbedder, self).__init__()    \n",
    "#         self.LSTM_stack = nn.LSTM(hp.data.nmels, hp.model.hidden, num_layers=hp.model.num_layer, batch_first=True)\n",
    "#         for name, param in self.LSTM_stack.named_parameters():\n",
    "#           if 'bias' in name:\n",
    "#              nn.init.constant_(param, 0.0)\n",
    "#           elif 'weight' in name:\n",
    "#              nn.init.xavier_normal_(param)\n",
    "#         self.projection = nn.Linear(hp.model.hidden, hp.model.proj)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n",
    "#         #only use last frame\n",
    "#         x = x[:,x.size(1)-1]\n",
    "#         x = self.projection(x.float())\n",
    "#         x = x / torch.norm(x, dim=1).unsqueeze(1)\n",
    "#         return x\n",
    "\n",
    "# embedder_net = SpeechEmbedder()\n",
    "# # embedder_net.load_state_dict(torch.load(hp.model.model_path))\n",
    "# embedder_net.eval()\n",
    "\n",
    "# times, segs = VAD_chunk(2, audio_path)\n",
    "# concat_seg = concat_segs(times, segs)\n",
    "# STFT_frames = get_STFTs(concat_seg)\n",
    "# STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "# STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "\n",
    "# embeddings = embedder_net(STFT_frames)\n",
    "\n",
    "# embeddings.shape\n",
    "\n",
    "# STFT_frames.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
